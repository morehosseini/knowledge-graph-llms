{"cells":[{"cell_type":"markdown","source":["**Using GPT4o**"],"metadata":{"id":"U01bf_AdEsiY"}},{"source":["# Cell 1: Install Required Libraries (Updated for GPT-4o and Data Validation)\n","!pip install langchain langchain-openai langchain-neo4j pandas pyvis langchain_experimental\n","!pip install openai  # For direct OpenAI API access\n","!pip install python-dotenv  # For environment variable management (optional)\n","\n","print(\"‚úÖ All required libraries installed successfully!\")\n","print(\"üìã Installed packages:\")\n","print(\"   - langchain (core framework)\")\n","print(\"   - langchain-openai (GPT-4o integration)\")\n","print(\"   - langchain-neo4j (Neo4j integration)\")\n","print(\"   - pandas (data manipulation)\")\n","print(\"   - pyvis (graph visualization)\")\n","print(\"   - langchain_experimental (graph transformers)\")\n","print(\"   - openai (direct API access)\")"],"cell_type":"code","metadata":{"id":"I96GJCJvVQ_5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1751181736256,"user_tz":-600,"elapsed":21091,"user":{"displayName":"M. Reza Hosseini","userId":"13449621777993109619"}},"outputId":"d1852576-e667-497d-d365-e020d12eb7f9"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n","Requirement already satisfied: langchain-openai in /usr/local/lib/python3.11/dist-packages (0.3.27)\n","Requirement already satisfied: langchain-neo4j in /usr/local/lib/python3.11/dist-packages (0.4.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: pyvis in /usr/local/lib/python3.11/dist-packages (0.3.2)\n","Requirement already satisfied: langchain_experimental in /usr/local/lib/python3.11/dist-packages (0.3.4)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.66)\n","Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n","Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.1)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n","Requirement already satisfied: openai<2.0.0,>=1.86.0 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.91.0)\n","Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n","Requirement already satisfied: neo4j<6.0.0,>=5.25.0 in /usr/local/lib/python3.11/dist-packages (from langchain-neo4j) (5.28.1)\n","Requirement already satisfied: neo4j-graphrag<2.0.0,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from langchain-neo4j) (1.7.0)\n","Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from pyvis) (7.34.0)\n","Requirement already satisfied: jinja2>=2.9.6 in /usr/local/lib/python3.11/dist-packages (from pyvis) (3.1.6)\n","Requirement already satisfied: jsonpickle>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from pyvis) (4.1.1)\n","Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.11/dist-packages (from pyvis) (3.5)\n","Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain_experimental) (0.3.26)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (75.2.0)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (0.19.2)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (0.7.5)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (5.7.1)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (3.0.51)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (2.19.2)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (0.1.7)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (4.9.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.9.6->pyvis) (3.0.2)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.11.15)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (8.5.0)\n","Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.6.7)\n","Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.10.1)\n","Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.4.1)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (24.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.14.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n","Requirement already satisfied: fsspec<2025.0.0,>=2024.9.0 in /usr/local/lib/python3.11/dist-packages (from neo4j-graphrag<2.0.0,>=1.5.0->langchain-neo4j) (2024.12.0)\n","Requirement already satisfied: json-repair<0.40.0,>=0.39.1 in /usr/local/lib/python3.11/dist-packages (from neo4j-graphrag<2.0.0,>=1.5.0->langchain-neo4j) (0.39.1)\n","Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from neo4j-graphrag<2.0.0,>=1.5.0->langchain-neo4j) (5.6.1)\n","Requirement already satisfied: types-pyyaml<7.0.0.0,>=6.0.12.20240917 in /usr/local/lib/python3.11/dist-packages (from neo4j-graphrag<2.0.0,>=1.5.0->langchain-neo4j) (6.0.12.20250516)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.9.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.10.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.67.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.6.15)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (6.4.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.20.1)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.26.1)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.9.0)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.4)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=5.3.0->pyvis) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->pyvis) (0.2.13)\n","Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.1.1)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.1.0)\n","Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.91.0)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n","Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.0)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.6.15)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n","Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.1)\n","‚úÖ All required libraries installed successfully!\n","üìã Installed packages:\n","   - langchain (core framework)\n","   - langchain-openai (GPT-4o integration)\n","   - langchain-neo4j (Neo4j integration)\n","   - pandas (data manipulation)\n","   - pyvis (graph visualization)\n","   - langchain_experimental (graph transformers)\n","   - openai (direct API access)\n"]}]},{"cell_type":"markdown","source":["**Cell 2: Full Setup (API Key, LLM, Transformer, Neo4j)**\n","\n","This is the main configuration cell. It will:\n","\n","Import all necessary modules.\n","Load your Gemini API Key from Colab's secrets.\n","Initialize the Gemini LLM.\n","Configure the connection to your Neo4j database.\n","Set up the LLMGraphTransformer with the strict schema we defined for high-quality data extraction.\n"],"metadata":{"id":"TVzrWnq2bp9N"}},{"source":["# Cell 2: Full Setup (GPT-4o-mini, Complete Clean Neo4j)\n","\n","import os\n","import asyncio\n","import pandas as pd\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# LangChain Imports\n","from langchain_core.documents import Document\n","from langchain.prompts import PromptTemplate\n","from langchain_experimental.graph_transformers import LLMGraphTransformer\n","from langchain_openai import ChatOpenAI  # Changed from Google Gemini to OpenAI\n","from langchain_neo4j import Neo4jGraph\n","\n","# Visualization Imports\n","from pyvis.network import Network\n","\n","print(\"üöÄ Starting Full Setup...\")\n","\n","# --- 1. Load OpenAI API Key from Colab Secrets ---\n","try:\n","    from google.colab import userdata\n","    openai_api_key = userdata.get('OPENAI_API_KEY')\n","    print(\"‚úÖ OpenAI API key retrieved from Colab secrets\")\n","except Exception as e:\n","    print(f\"‚ùå Error retrieving OpenAI API key: {e}\")\n","    openai_api_key = None\n","\n","# --- 2. Initialize GPT-4o-mini ---\n","if openai_api_key:\n","    try:\n","        llm = ChatOpenAI(\n","            model=\"gpt-4o-mini\",  # Using gpt-4o-mini as requested\n","            temperature=0,        # Deterministic responses\n","            openai_api_key=openai_api_key,\n","            max_tokens=4000      # Sufficient for graph extraction\n","        )\n","        print(\"‚úÖ GPT-4o-mini initialized successfully\")\n","    except Exception as e:\n","        print(f\"‚ùå Error initializing GPT-4o-mini: {e}\")\n","        llm = None\n","else:\n","    print(\"‚ùå Cannot initialize GPT-4o-mini without API key\")\n","    llm = None\n","\n","# --- 3. Neo4j Database Connection ---\n","NEO4J_URI = \"neo4j+s://15286d63.databases.neo4j.io\"\n","NEO4J_USERNAME = \"neo4j\"\n","NEO4J_PASSWORD = \"VvdcFiV8FfMzGFILRMuocittAww1QzMom1zfGPwVCuA\"\n","\n","os.environ[\"NEO4J_URI\"] = NEO4J_URI\n","os.environ[\"NEO4J_USERNAME\"] = NEO4J_USERNAME\n","os.environ[\"NEO4J_PASSWORD\"] = NEO4J_PASSWORD\n","\n","try:\n","    graph = Neo4jGraph()\n","    print(\"‚úÖ Neo4j connection established\")\n","\n","    # Test connection\n","    result = graph.query(\"RETURN 'Connection successful' as status\")\n","    print(f\"‚úÖ Neo4j connection verified: {result[0]['status']}\")\n","\n","except Exception as e:\n","    print(f\"‚ùå Neo4j connection failed: {e}\")\n","    graph = None\n","\n","# --- 4. üßπ COMPLETE ROOM CLEANING (Delete Everything) ---\n","if graph:\n","    try:\n","        print(\"\\nüßπ Starting COMPLETE database cleaning...\")\n","\n","        # Count existing data before cleaning\n","        node_count = graph.query(\"MATCH (n) RETURN count(n) as count\")[0]['count']\n","        rel_count = graph.query(\"MATCH ()-[r]-() RETURN count(r) as count\")[0]['count']\n","        print(f\"üìä Before cleaning: {node_count} nodes, {rel_count} relationships\")\n","\n","        # COMPLETE CLEANING - Delete everything\n","        graph.query(\"MATCH (n) DETACH DELETE n\")\n","\n","        # Verify cleaning worked\n","        node_count_after = graph.query(\"MATCH (n) RETURN count(n) as count\")[0]['count']\n","        rel_count_after = graph.query(\"MATCH ()-[r]-() RETURN count(r) as count\")[0]['count']\n","\n","        print(f\"‚úÖ Cleaning complete: {node_count_after} nodes, {rel_count_after} relationships\")\n","        print(\"üéâ Database is now completely clean and ready for fresh data!\")\n","\n","    except Exception as e:\n","        print(f\"‚ùå Error during database cleaning: {e}\")\n","\n","# --- 5. Create Fresh Constraints for Data Integrity ---\n","if graph:\n","    try:\n","        print(\"\\nüîß Creating fresh data integrity constraints...\")\n","\n","        constraints = [\n","            \"CREATE CONSTRAINT role_id_unique IF NOT EXISTS FOR (r:Role) REQUIRE r.id IS UNIQUE\",\n","            \"CREATE CONSTRAINT skill_id_unique IF NOT EXISTS FOR (s:`Technical skill`) REQUIRE s.id IS UNIQUE\",\n","            \"CREATE CONSTRAINT soft_skill_id_unique IF NOT EXISTS FOR (s:`Soft skill`) REQUIRE s.id IS UNIQUE\",\n","            \"CREATE CONSTRAINT tool_id_unique IF NOT EXISTS FOR (t:Tool) REQUIRE t.id IS UNIQUE\",\n","            \"CREATE CONSTRAINT task_id_unique IF NOT EXISTS FOR (t:Task) REQUIRE t.id IS UNIQUE\"\n","        ]\n","\n","        for constraint in constraints:\n","            try:\n","                graph.query(constraint)\n","                print(f\"‚úÖ Created constraint: {constraint.split('FOR')[1].split('REQUIRE')[0].strip()}\")\n","            except Exception as e:\n","                if \"already exists\" in str(e).lower():\n","                    print(f\"‚ÑπÔ∏è  Constraint already exists (OK)\")\n","                else:\n","                    print(f\"‚ö†Ô∏è  Constraint creation issue: {e}\")\n","\n","    except Exception as e:\n","        print(f\"‚ùå Error creating constraints: {e}\")\n","\n","# --- 6. Configure Graph Transformer with Clean Schema ---\n","allowed_nodes_schema = [\"Role\", \"Technical skill\", \"Soft skill\", \"Tool\", \"Task\"]\n","allowed_relationships_schema = [\n","    \"REQUIRES_SKILL\",      # Role -> Technical Skill or Soft Skill\n","    \"USES_TOOL\",          # Role -> Tool\n","    \"PERFORMS_TASK\",      # Role -> Task\n","    \"INVOLVES_SKILL\",     # Task -> Technical Skill or Soft Skill\n","    \"REQUIRES_TOOL\",      # Task -> Tool\n","    \"REQUIRES_EXPERIENCE\" # Role -> Technical Skill/Tool (when experience is mentioned)\n","]\n","\n","CYPHER_GENERATION_TEMPLATE = f\"\"\"\n","You are an expert in analyzing job descriptions to extract competency requirements.\n","Focus ONLY on roles, tasks, skills, and tools - ignore company names and locations completely.\n","\n","From the provided text, extract entities and relationships that conform to this schema:\n","\n","Schema:\n","- Allowed Node Labels: {', '.join(allowed_nodes_schema)}\n","- Allowed Relationship Types: {', '.join(allowed_relationships_schema)}\n","\n","Node Definitions:\n","- Role: Job titles or positions (e.g., \"Data Scientist\", \"ML Engineer\", \"AI Researcher\")\n","- Task: Specific job responsibilities or activities (e.g., \"Build machine learning models\", \"Analyze data\", \"Deploy solutions\")\n","- Technical Skill: Programming languages, frameworks, technical methodologies (e.g., \"Python\", \"TensorFlow\", \"Machine Learning\")\n","- Soft Skill: Interpersonal or general professional skills (e.g., \"Communication\", \"Leadership\", \"Problem-solving\")\n","- Tool: Specific software, platforms, or applications (e.g., \"AWS\", \"Docker\", \"Tableau\", \"Git\")\n","\n","Relationship Guidelines:\n","- REQUIRES_SKILL: When a role requires a specific technical or soft skill\n","- USES_TOOL: When a role uses specific tools or platforms\n","- PERFORMS_TASK: When a role is responsible for specific tasks\n","- INVOLVES_SKILL: When a task requires or involves specific skills\n","- REQUIRES_TOOL: When a task requires specific tools\n","- REQUIRES_EXPERIENCE: When specific years of experience with a skill/tool is mentioned\n","\n","Important:\n","1. DO NOT create nodes for company names, locations, or any other entity types\n","2. Extract the actual job title as Role, not generic terms\n","3. Be specific with skills and tools - use the exact names mentioned\n","4. Distinguish between skills (knowledge/ability) and tools (software/platforms)\n","5. Each node must have a valid, non-null 'id' property\n","\n","Text:\n","{{input}}\n","\n","Cypher Query:\n","\"\"\"\n","\n","if llm and graph:\n","    try:\n","        prompt = PromptTemplate.from_template(CYPHER_GENERATION_TEMPLATE)\n","        graph_transformer = LLMGraphTransformer(\n","            llm=llm,\n","            prompt=prompt,\n","            allowed_nodes=allowed_nodes_schema,\n","            allowed_relationships=allowed_relationships_schema\n","        )\n","        print(\"‚úÖ Graph transformer configured with clean schema\")\n","    except Exception as e:\n","        print(f\"‚ùå Error configuring graph transformer: {e}\")\n","        graph_transformer = None\n","else:\n","    print(\"‚ùå Cannot configure graph transformer - missing LLM or Graph connection\")\n","    graph_transformer = None\n","\n","# --- 7. Final Status Report ---\n","print(\"\\n\" + \"=\"*60)\n","print(\"üìã SETUP COMPLETE - STATUS REPORT\")\n","print(\"=\"*60)\n","print(f\"ü§ñ GPT-4o-mini: {'‚úÖ Ready' if llm else '‚ùå Failed'}\")\n","print(f\"üóÑÔ∏è  Neo4j Graph: {'‚úÖ Connected & Clean' if graph else '‚ùå Failed'}\")\n","print(f\"üîß Graph Transformer: {'‚úÖ Configured' if graph_transformer else '‚ùå Failed'}\")\n","print(f\"üßπ Database Status: {'‚úÖ Completely Clean' if graph else '‚ùå Unknown'}\")\n","\n","if llm and graph and graph_transformer:\n","    print(\"\\nüéâ ALL SYSTEMS READY! You can proceed to Cell 3.\")\n","else:\n","    print(\"\\n‚ö†Ô∏è  Some components failed. Check errors above before proceeding.\")"],"cell_type":"code","metadata":{"id":"Qr_ZYGlBUsVY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1751181845594,"user_tz":-600,"elapsed":31203,"user":{"displayName":"M. Reza Hosseini","userId":"13449621777993109619"}},"outputId":"6fba2d32-6424-4492-bac7-6f77d4567be4"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["üöÄ Starting Full Setup...\n","‚úÖ OpenAI API key retrieved from Colab secrets\n","‚úÖ GPT-4o-mini initialized successfully\n","‚úÖ Neo4j connection established\n","‚úÖ Neo4j connection verified: Connection successful\n","\n","üßπ Starting COMPLETE database cleaning...\n","üìä Before cleaning: 0 nodes, 0 relationships\n","‚úÖ Cleaning complete: 0 nodes, 0 relationships\n","üéâ Database is now completely clean and ready for fresh data!\n","\n","üîß Creating fresh data integrity constraints...\n","‚úÖ Created constraint: (r:Role)\n","‚úÖ Created constraint: (s:`Technical skill`)\n","‚úÖ Created constraint: (s:`Soft skill`)\n","‚úÖ Created constraint: (t:Tool)\n","‚úÖ Created constraint: (t:Task)\n","‚úÖ Graph transformer configured with clean schema\n","\n","============================================================\n","üìã SETUP COMPLETE - STATUS REPORT\n","============================================================\n","ü§ñ GPT-4o-mini: ‚úÖ Ready\n","üóÑÔ∏è  Neo4j Graph: ‚úÖ Connected & Clean\n","üîß Graph Transformer: ‚úÖ Configured\n","üßπ Database Status: ‚úÖ Completely Clean\n","\n","üéâ ALL SYSTEMS READY! You can proceed to Cell 3.\n"]}]},{"cell_type":"markdown","source":["**Cell 3: Load Your CSV Data**\n","\n","Please replace the placeholder values with the path to your CSV file and the name of the column that contains the job description text."],"metadata":{"id":"FiObhYuhua-6"}},{"cell_type":"code","source":["# Cell 3: Load Test CSV Data (4 samples)\n","\n","import pandas as pd\n","from google.colab import drive\n","from langchain_core.documents import Document\n","\n","print(\"üìÅ Loading your test CSV data...\")\n","\n","# --- 1. Mount Google Drive (if not already mounted) ---\n","try:\n","    drive.mount('/content/drive', force_remount=False)\n","    print(\"‚úÖ Google Drive mounted successfully\")\n","except Exception as e:\n","    print(f\"‚ö†Ô∏è  Drive mount issue (might already be mounted): {e}\")\n","\n","# --- 2. Load your test CSV file ---\n","try:\n","    # Update this path to match where you uploaded Book1.csv\n","    csv_path = \"/content/drive/MyDrive/knowledge-graph-llms/Book1.csv\"  # Adjust path as needed\n","\n","    df = pd.read_csv(csv_path)\n","    print(f\"‚úÖ CSV loaded successfully: {len(df)} rows, {len(df.columns)} columns\")\n","\n","    # Show column names\n","    print(f\"üìã Columns: {list(df.columns)}\")\n","\n","except Exception as e:\n","    print(f\"‚ùå Error loading CSV: {e}\")\n","    print(\"üîß Please check the file path. Common locations:\")\n","    print(\"   - /content/drive/MyDrive/knowledge-graph-llms/Book1.csv\")\n","    print(\"   - /content/drive/MyDrive/YourFolderName/Book1.csv\")\n","    df = None\n","\n","# --- 3. Examine the data structure ---\n","if df is not None:\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"üìä DATA OVERVIEW\")\n","    print(\"=\"*60)\n","\n","    # Show basic info\n","    print(f\"Dataset shape: {df.shape}\")\n","    print(f\"Key columns for graph extraction:\")\n","    print(f\"  - Title: {df['Title'].notna().sum()}/{len(df)} non-null values\")\n","    print(f\"  - JobText: {df['JobText'].notna().sum()}/{len(df)} non-null values\")\n","\n","    # Show sample of the data\n","    print(\"\\nüìù SAMPLE DATA:\")\n","    for i, row in df.head(4).iterrows():\n","        print(f\"\\n--- Job {i+1} ---\")\n","        print(f\"Title: {row['Title']}\")\n","        print(f\"Employer: {row['Employer']}\")\n","        print(f\"JobText (first 200 chars): {str(row['JobText'])[:200]}...\")\n","        print(f\"JobText length: {len(str(row['JobText']))} characters\")\n","\n","# --- 4. Create Documents for Graph Transformer ---\n","documents = []\n","if df is not None:\n","    try:\n","        print(\"\\nüîÑ Converting to LangChain Document format...\")\n","\n","        for i, row in df.iterrows():\n","            # Combine title and job text for better context\n","            content = f\"Job Title: {row['Title']}\\n\\nJob Description:\\n{row['JobText']}\"\n","\n","            # Create metadata\n","            metadata = {\n","                'job_id': row['JobID'],\n","                'title': row['Title'],\n","                'employer': row['Employer'],\n","                'city': row['City'],\n","                'state': row['State'],\n","                'source': 'test_csv'\n","            }\n","\n","            # Create LangChain Document\n","            doc = Document(\n","                page_content=content,\n","                metadata=metadata\n","            )\n","            documents.append(doc)\n","\n","        print(f\"‚úÖ Created {len(documents)} documents for graph extraction\")\n","\n","        # Show sample document\n","        print(f\"\\nüìã SAMPLE DOCUMENT FORMAT:\")\n","        print(f\"Content preview: {documents[0].page_content[:300]}...\")\n","        print(f\"Metadata: {documents[0].metadata}\")\n","\n","    except Exception as e:\n","        print(f\"‚ùå Error creating documents: {e}\")\n","        documents = []\n","\n","# --- 5. Validation and Ready Check ---\n","print(\"\\n\" + \"=\"*60)\n","print(\"üîç VALIDATION REPORT\")\n","print(\"=\"*60)\n","\n","validation_passed = True\n","\n","# Check 1: Data loaded\n","if df is None:\n","    print(\"‚ùå CSV data not loaded\")\n","    validation_passed = False\n","else:\n","    print(f\"‚úÖ CSV data loaded: {len(df)} rows\")\n","\n","# Check 2: Required columns exist\n","if df is not None:\n","    required_columns = ['Title', 'JobText']\n","    missing_columns = [col for col in required_columns if col not in df.columns]\n","    if missing_columns:\n","        print(f\"‚ùå Missing required columns: {missing_columns}\")\n","        validation_passed = False\n","    else:\n","        print(\"‚úÖ All required columns present\")\n","\n","# Check 3: Documents created\n","if len(documents) == 0:\n","    print(\"‚ùå No documents created for graph extraction\")\n","    validation_passed = False\n","else:\n","    print(f\"‚úÖ Documents ready: {len(documents)} job postings\")\n","\n","# Check 4: Content quality\n","if documents:\n","    avg_length = sum(len(doc.page_content) for doc in documents) / len(documents)\n","    if avg_length < 100:\n","        print(f\"‚ö†Ô∏è  Warning: Average content length is short ({avg_length:.0f} chars)\")\n","    else:\n","        print(f\"‚úÖ Content quality good: Average {avg_length:.0f} characters per job\")\n","\n","# Final status\n","print(\"\\n\" + \"=\"*60)\n","if validation_passed:\n","    print(\"üéâ CELL 3 COMPLETE - DATA READY FOR GRAPH EXTRACTION!\")\n","    print(\"‚úÖ You can proceed to Cell 4 to create the knowledge graph\")\n","    print(f\"üìä Ready to process: {len(documents)} job postings\")\n","else:\n","    print(\"‚ö†Ô∏è  Please fix the issues above before proceeding to Cell 4\")\n","\n","print(\"=\"*60)"],"metadata":{"id":"RkMYsNQguTJ_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1751011509303,"user_tz":-600,"elapsed":20436,"user":{"displayName":"M. Reza Hosseini","userId":"13449621777993109619"}},"outputId":"fa28f53e-c9de-46d2-d194-436fb9837b9b"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["üìÅ Loading your test CSV data...\n","Mounted at /content/drive\n","‚úÖ Google Drive mounted successfully\n","‚úÖ CSV loaded successfully: 4 rows, 11 columns\n","üìã Columns: ['Employer', 'JobID', 'City', 'State', 'Title', 'JobDate', 'ANZSIC CODE', 'ANZSCO CODE', 'LOT CODE', 'JobText', 'JobUrl']\n","\n","============================================================\n","üìä DATA OVERVIEW\n","============================================================\n","Dataset shape: (4, 11)\n","Key columns for graph extraction:\n","  - Title: 4/4 non-null values\n","  - JobText: 4/4 non-null values\n","\n","üìù SAMPLE DATA:\n","\n","--- Job 1 ---\n","Title: Analytics and Insights Managers\n","Employer: Mirvac\n","JobText (first 200 chars): * Company Mirvac Group -\n","    Description\n","    \n","    Our customer data is a valued asset that blends internal and external data sources, Voice of Customer feedback and systemically records customer intel...\n","JobText length: 1604 characters\n","\n","--- Job 2 ---\n","Title: Automation Managers\n","Employer: Mirvac\n","JobText (first 200 chars): 1006\n","    \n","    Solution Engineer, Intelligent Process Automation\n","    \n","    Sydney, AU-NSW, Australia\n","    \n","    |\n","    \n","    req791\n","    \n","    \n","    \n","    Mirvac is a leading, diversified Australian property gr...\n","JobText length: 3979 characters\n","\n","--- Job 3 ---\n","Title: Mechanical Service Technicians\n","Employer: Johnathan Thurston Academy\n","JobText (first 200 chars): Service Technician Mechanical - Darwin\n","    \n","    Johnathan Thurston Academy\n","    \n","    Darwin NT\n","    \n","    Johnathan Thurston Academy\n","    \n","    Taking employment back to basics. We simply introduce jobseek...\n","JobText length: 3973 characters\n","\n","--- Job 4 ---\n","Title: Data Engineers\n","Employer: Ventia\n","JobText (first 200 chars): Data Engineer\n","    \n","    \n","    \n","    \n","    \n","    \n","    \n","    Date: 11-Mar-2021\n","    \n","    Location: Baulkam Hills, NSW, AU, 2153\n","    \n","    Company: Ventia\n","    \n","    One of Australasia's largest dedicated infrastr...\n","JobText length: 3224 characters\n","\n","üîÑ Converting to LangChain Document format...\n","‚úÖ Created 4 documents for graph extraction\n","\n","üìã SAMPLE DOCUMENT FORMAT:\n","Content preview: Job Title: Analytics and Insights Managers\n","\n","Job Description:\n","* Company Mirvac Group -\n","    Description\n","    \n","    Our customer data is a valued asset that blends internal and external data sources, Voice of Customer feedback and systemically records customer intelligence from customer interactions in o...\n","Metadata: {'job_id': '37716b56223ca91a66bc3600c87bdbc62053504c', 'title': 'Analytics and Insights Managers', 'employer': 'Mirvac', 'city': 'Sydney, AUS', 'state': 'New South Wales', 'source': 'test_csv'}\n","\n","============================================================\n","üîç VALIDATION REPORT\n","============================================================\n","‚úÖ CSV data loaded: 4 rows\n","‚úÖ All required columns present\n","‚úÖ Documents ready: 4 job postings\n","‚úÖ Content quality good: Average 3248 characters per job\n","\n","============================================================\n","üéâ CELL 3 COMPLETE - DATA READY FOR GRAPH EXTRACTION!\n","‚úÖ You can proceed to Cell 4 to create the knowledge graph\n","üìä Ready to process: 4 job postings\n","============================================================\n"]}]},{"cell_type":"markdown","source":["**Cell 4: Process Documents and Create Knowledge Graph**\n","Here's what Cell 4 will do:\n","Process:\n","\n","Process all 4 jobs simultaneously using GPT-4o-mini\n","Extract entities and relationships for each job posting\n","Add nodes and relationships to your clean Neo4j database\n","Show summary statistics and sample extractions\n","Create a simple visualization of the resulting graph\n","Validate the graph structure is correct\n","\n"],"metadata":{"id":"KEAH0XNJDajk"}},{"cell_type":"code","source":["# Cell 4: Process Documents and Create Knowledge Graph\n","\n","import time\n","from collections import Counter\n","import json\n","\n","print(\"üöÄ Starting Knowledge Graph Extraction...\")\n","print(\"üìã Processing 4 job postings with GPT-4o-mini\")\n","\n","# --- 1. Verify Prerequisites ---\n","prerequisites_check = True\n","\n","if 'documents' not in locals() or len(documents) == 0:\n","    print(\"‚ùå No documents found. Please run Cell 3 first.\")\n","    prerequisites_check = False\n","\n","if 'graph_transformer' not in locals() or graph_transformer is None:\n","    print(\"‚ùå Graph transformer not available. Please run Cell 2 first.\")\n","    prerequisites_check = False\n","\n","if 'graph' not in locals() or graph is None:\n","    print(\"‚ùå Neo4j graph not available. Please run Cell 2 first.\")\n","    prerequisites_check = False\n","\n","if not prerequisites_check:\n","    print(\"‚ö†Ô∏è  Cannot proceed. Please run previous cells first.\")\n","else:\n","    print(\"‚úÖ All prerequisites met. Starting extraction...\")\n","\n","# --- 2. Process Documents and Extract Graph ---\n","if prerequisites_check:\n","    try:\n","        print(f\"\\nüîÑ Extracting knowledge graph from {len(documents)} job postings...\")\n","        start_time = time.time()\n","\n","        # Extract graph documents using the transformer\n","        graph_documents = graph_transformer.convert_to_graph_documents(documents)\n","\n","        extraction_time = time.time() - start_time\n","        print(f\"‚úÖ Graph extraction completed in {extraction_time:.1f} seconds\")\n","        print(f\"üìä Generated {len(graph_documents)} graph documents\")\n","\n","    except Exception as e:\n","        print(f\"‚ùå Error during graph extraction: {e}\")\n","        graph_documents = []\n","\n","# --- 3. Analyze Extracted Entities (Before Adding to Neo4j) ---\n","if graph_documents:\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"üìä EXTRACTION ANALYSIS\")\n","    print(\"=\"*60)\n","\n","    # Collect all entities and relationships\n","    all_nodes = []\n","    all_relationships = []\n","\n","    for graph_doc in graph_documents:\n","        all_nodes.extend(graph_doc.nodes)\n","        all_relationships.extend(graph_doc.relationships)\n","\n","    # Count by type\n","    node_types = Counter([node.type for node in all_nodes])\n","    relationship_types = Counter([rel.type for rel in all_relationships])\n","\n","    print(f\"üìà EXTRACTED ENTITIES:\")\n","    print(f\"  Total Nodes: {len(all_nodes)}\")\n","    for node_type, count in node_types.most_common():\n","        print(f\"    - {node_type}: {count}\")\n","\n","    print(f\"\\nüîó EXTRACTED RELATIONSHIPS:\")\n","    print(f\"  Total Relationships: {len(all_relationships)}\")\n","    for rel_type, count in relationship_types.most_common():\n","        print(f\"    - {rel_type}: {count}\")\n","\n","    # Show sample entities\n","    print(f\"\\nüìù SAMPLE EXTRACTED ENTITIES:\")\n","    for node_type in ['Role', 'Technical skill', 'Soft skill', 'Tool', 'Task']:\n","        sample_nodes = [node.id for node in all_nodes if node.type == node_type][:3]\n","        if sample_nodes:\n","            print(f\"  {node_type}: {', '.join(sample_nodes)}\")\n","\n","# --- 4. Add Graph Documents to Neo4j ---\n","if graph_documents:\n","    try:\n","        print(f\"\\nüíæ Adding extracted graph to Neo4j database...\")\n","\n","        # Add to Neo4j\n","        graph.add_graph_documents(graph_documents)\n","\n","        print(\"‚úÖ Graph successfully added to Neo4j!\")\n","\n","        # Verify what was added\n","        verification_query = \"\"\"\n","        MATCH (n)\n","        RETURN labels(n)[0] as node_type, count(n) as count\n","        ORDER BY count DESC\n","        \"\"\"\n","\n","        db_stats = graph.query(verification_query)\n","\n","        print(f\"\\nüìä NEO4J DATABASE STATISTICS:\")\n","        total_nodes = sum([stat['count'] for stat in db_stats])\n","        print(f\"  Total Nodes in Database: {total_nodes}\")\n","\n","        for stat in db_stats:\n","            print(f\"    - {stat['node_type']}: {stat['count']}\")\n","\n","        # Count relationships\n","        rel_count_query = \"MATCH ()-[r]-() RETURN count(r) as count\"\n","        rel_count = graph.query(rel_count_query)[0]['count']\n","        print(f\"  Total Relationships: {rel_count}\")\n","\n","    except Exception as e:\n","        print(f\"‚ùå Error adding graph to Neo4j: {e}\")\n","\n","# --- 5. Sample Graph Queries for Validation ---\n","if graph_documents:\n","    print(f\"\\nüîç SAMPLE GRAPH QUERIES:\")\n","\n","    try:\n","        # Query 1: Find all roles and their required skills\n","        roles_skills_query = \"\"\"\n","        MATCH (r:Role)-[:REQUIRES_SKILL]->(s)\n","        WHERE labels(s)[0] IN ['Technical skill', 'Soft skill']\n","        RETURN r.id as role, labels(s)[0] as skill_type, s.id as skill\n","        LIMIT 10\n","        \"\"\"\n","\n","        roles_skills = graph.query(roles_skills_query)\n","        if roles_skills:\n","            print(\"  üéØ Roles and Required Skills:\")\n","            for item in roles_skills[:5]:  # Show first 5\n","                print(f\"    - {item['role']} requires {item['skill_type']}: {item['skill']}\")\n","\n","        # Query 2: Find all tools used by roles\n","        roles_tools_query = \"\"\"\n","        MATCH (r:Role)-[:USES_TOOL]->(t:Tool)\n","        RETURN r.id as role, t.id as tool\n","        LIMIT 5\n","        \"\"\"\n","\n","        roles_tools = graph.query(roles_tools_query)\n","        if roles_tools:\n","            print(\"  üõ†Ô∏è  Roles and Tools:\")\n","            for item in roles_tools:\n","                print(f\"    - {item['role']} uses {item['tool']}\")\n","\n","        # Query 3: Find tasks and their required skills\n","        tasks_skills_query = \"\"\"\n","        MATCH (t:Task)-[:INVOLVES_SKILL]->(s)\n","        WHERE labels(s)[0] IN ['Technical skill', 'Soft skill']\n","        RETURN t.id as task, s.id as skill\n","        LIMIT 5\n","        \"\"\"\n","\n","        tasks_skills = graph.query(tasks_skills_query)\n","        if tasks_skills:\n","            print(\"  üìã Tasks and Skills:\")\n","            for item in tasks_skills:\n","                print(f\"    - Task '{item['task']}' involves {item['skill']}\")\n","\n","    except Exception as e:\n","        print(f\"‚ö†Ô∏è  Error running sample queries: {e}\")\n","\n","# --- 6. Create Simple Graph Visualization ---\n","if graph_documents:\n","    try:\n","        print(f\"\\nüé® Creating graph visualization...\")\n","\n","        # Get sample data for visualization (limited to avoid clutter)\n","        viz_query = \"\"\"\n","        MATCH (n)-[r]->(m)\n","        RETURN n.id as source, labels(n)[0] as source_type,\n","               type(r) as relation,\n","               m.id as target, labels(m)[0] as target_type\n","        LIMIT 20\n","        \"\"\"\n","\n","        viz_data = graph.query(viz_query)\n","\n","        if viz_data:\n","            # Create network visualization\n","            net = Network(height=\"400px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\")\n","\n","            # Add nodes and edges\n","            added_nodes = set()\n","\n","            for item in viz_data:\n","                # Add source node\n","                if item['source'] not in added_nodes:\n","                    color = {\n","                        'Role': '#FF6B6B',\n","                        'Technical skill': '#4ECDC4',\n","                        'Soft skill': '#45B7D1',\n","                        'Tool': '#96CEB4',\n","                        'Task': '#FECA57'\n","                    }.get(item['source_type'], '#DDA0DD')\n","\n","                    net.add_node(item['source'],\n","                                label=item['source'][:30] + ('...' if len(item['source']) > 30 else ''),\n","                                color=color,\n","                                title=f\"{item['source_type']}: {item['source']}\")\n","                    added_nodes.add(item['source'])\n","\n","                # Add target node\n","                if item['target'] not in added_nodes:\n","                    color = {\n","                        'Role': '#FF6B6B',\n","                        'Technical skill': '#4ECDC4',\n","                        'Soft skill': '#45B7D1',\n","                        'Tool': '#96CEB4',\n","                        'Task': '#FECA57'\n","                    }.get(item['target_type'], '#DDA0DD')\n","\n","                    net.add_node(item['target'],\n","                                label=item['target'][:30] + ('...' if len(item['target']) > 30 else ''),\n","                                color=color,\n","                                title=f\"{item['target_type']}: {item['target']}\")\n","                    added_nodes.add(item['target'])\n","\n","                # Add edge\n","                net.add_edge(item['source'], item['target'],\n","                            label=item['relation'],\n","                            title=item['relation'])\n","\n","            # Save and display\n","            net.save_graph(\"/content/knowledge_graph_sample.html\")\n","            print(\"‚úÖ Graph visualization saved as 'knowledge_graph_sample.html'\")\n","            print(\"üîó You can download and open this file to view the interactive graph\")\n","\n","        else:\n","            print(\"‚ö†Ô∏è  No relationships found for visualization\")\n","\n","    except Exception as e:\n","        print(f\"‚ö†Ô∏è  Visualization creation failed: {e}\")\n","\n","# --- 7. Final Summary Report ---\n","print(\"\\n\" + \"=\"*60)\n","print(\"üéâ CELL 4 COMPLETE - KNOWLEDGE GRAPH CREATED!\")\n","print(\"=\"*60)\n","\n","if graph_documents:\n","    print(f\"‚úÖ Successfully processed {len(documents)} job postings\")\n","    print(f\"‚úÖ Extracted {len(all_nodes)} entities and {len(all_relationships)} relationships\")\n","    print(f\"‚úÖ Added all data to Neo4j database\")\n","    print(f\"‚úÖ Created sample visualization\")\n","\n","    print(f\"\\nüìä QUICK STATS:\")\n","    print(f\"  - Roles: {node_types.get('Role', 0)}\")\n","    print(f\"  - Technical Skills: {node_types.get('Technical skill', 0)}\")\n","    print(f\"  - Soft Skills: {node_types.get('Soft skill', 0)}\")\n","    print(f\"  - Tools: {node_types.get('Tool', 0)}\")\n","    print(f\"  - Tasks: {node_types.get('Task', 0)}\")\n","\n","    print(f\"\\nüéØ NEXT STEPS:\")\n","    print(f\"  1. Review the sample queries above\")\n","    print(f\"  2. Download and view the graph visualization\")\n","    print(f\"  3. Test with larger datasets if results look good\")\n","    print(f\"  4. Run your LLM classification analysis\")\n","\n","else:\n","    print(\"‚ùå Graph creation failed. Check errors above.\")\n","\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PvqZU0M0DXLv","executionInfo":{"status":"ok","timestamp":1751011610976,"user_tz":-600,"elapsed":73935,"user":{"displayName":"M. Reza Hosseini","userId":"13449621777993109619"}},"outputId":"700e5d30-b768-4c95-9ae9-3d8988a2a9c8"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["üöÄ Starting Knowledge Graph Extraction...\n","üìã Processing 4 job postings with GPT-4o-mini\n","‚úÖ All prerequisites met. Starting extraction...\n","\n","üîÑ Extracting knowledge graph from 4 job postings...\n","‚úÖ Graph extraction completed in 66.9 seconds\n","üìä Generated 4 graph documents\n","\n","============================================================\n","üìä EXTRACTION ANALYSIS\n","============================================================\n","üìà EXTRACTED ENTITIES:\n","  Total Nodes: 73\n","    - Task: 24\n","    - Technical skill: 17\n","    - Tool: 15\n","    - Soft skill: 13\n","    - Role: 4\n","\n","üîó EXTRACTED RELATIONSHIPS:\n","  Total Relationships: 83\n","    - REQUIRES_SKILL: 26\n","    - PERFORMS_TASK: 24\n","    - INVOLVES_SKILL: 12\n","    - REQUIRES_TOOL: 11\n","    - USES_TOOL: 10\n","\n","üìù SAMPLE EXTRACTED ENTITIES:\n","  Role: Analytics And Insights Manager, Automation Manager, Mechanical Service Technician\n","  Technical skill: Predictive Analytics, Machine Learning, C#\n","  Soft skill: Communication, Project Management, Attention To Detail\n","  Tool: Google Analytics, Tag Manager, Webmaster Tools\n","  Task: Market Research, Customer Data Analysis, Insight Generation\n","\n","üíæ Adding extracted graph to Neo4j database...\n","‚úÖ Graph successfully added to Neo4j!\n","\n","üìä NEO4J DATABASE STATISTICS:\n","  Total Nodes in Database: 71\n","    - Task: 24\n","    - Technical skill: 16\n","    - Tool: 15\n","    - Soft skill: 12\n","    - Role: 4\n","  Total Relationships: 166\n","\n","üîç SAMPLE GRAPH QUERIES:\n","  üéØ Roles and Required Skills:\n","    - Analytics And Insights Manager requires Technical skill: Predictive Analytics\n","    - Analytics And Insights Manager requires Technical skill: Machine Learning\n","    - Analytics And Insights Manager requires Soft skill: Communication\n","    - Analytics And Insights Manager requires Soft skill: Project Management\n","    - Automation Manager requires Technical skill: C#\n","  üõ†Ô∏è  Roles and Tools:\n","    - Analytics And Insights Manager uses Google Analytics\n","    - Analytics And Insights Manager uses Tag Manager\n","    - Analytics And Insights Manager uses Webmaster Tools\n","    - Analytics And Insights Manager uses Google Ads\n","    - Data Engineer uses Jupyter Notebooks\n","  üìã Tasks and Skills:\n","    - Task 'Market Research' involves Communication\n","    - Task 'Customer Data Analysis' involves Predictive Analytics\n","    - Task 'Insight Generation' involves Machine Learning\n","    - Task 'Commercial Air Conditioning Maintenance' involves Diagnostics\n","    - Task 'Commercial Air Conditioning Maintenance' involves Repairs\n","\n","üé® Creating graph visualization...\n","‚úÖ Graph visualization saved as 'knowledge_graph_sample.html'\n","üîó You can download and open this file to view the interactive graph\n","\n","============================================================\n","üéâ CELL 4 COMPLETE - KNOWLEDGE GRAPH CREATED!\n","============================================================\n","‚úÖ Successfully processed 4 job postings\n","‚úÖ Extracted 73 entities and 83 relationships\n","‚úÖ Added all data to Neo4j database\n","‚úÖ Created sample visualization\n","\n","üìä QUICK STATS:\n","  - Roles: 4\n","  - Technical Skills: 17\n","  - Soft Skills: 13\n","  - Tools: 15\n","  - Tasks: 24\n","\n","üéØ NEXT STEPS:\n","  1. Review the sample queries above\n","  2. Download and view the graph visualization\n","  3. Test with larger datasets if results look good\n","  4. Run your LLM classification analysis\n","============================================================\n"]}]},{"cell_type":"markdown","source":["**Cell 5A: Pure LLM Construction Industry Classification**"],"metadata":{"id":"p8MHRGi5QleR"}},{"cell_type":"code","source":["# Cell 5A: GPT-4o Enhanced Classification (Fixed Syntax Error)\n","\n","import pandas as pd\n","import time\n","import json\n","from datetime import datetime\n","from langchain_openai import ChatOpenAI\n","\n","print(\"üß† GPT-4o ENHANCED CONSTRUCTION + AI/ML ECOSYSTEM CLASSIFICATION\")\n","print(\"üéØ Research Focus: AI/Automation/Data Science in Construction\")\n","print(\"=\"*70)\n","\n","# --- 1. Setup Enhanced GPT-4o for Classification ---\n","try:\n","    from google.colab import userdata\n","    openai_api_key = userdata.get('OPENAI_API_KEY')\n","    print(\"‚úÖ OpenAI API key retrieved\")\n","except:\n","    print(\"‚ùå Could not retrieve OpenAI API key\")\n","    openai_api_key = None\n","\n","if openai_api_key:\n","    try:\n","        # Upgrade to GPT-4o for superior reasoning\n","        llm_classification = ChatOpenAI(\n","            model=\"gpt-4o\",  # Enhanced reasoning model\n","            temperature=0,   # Deterministic results\n","            openai_api_key=openai_api_key,\n","            max_tokens=150   # Enough for reasoning + classification\n","        )\n","        print(\"‚úÖ GPT-4o initialized for enhanced classification\")\n","    except Exception as e:\n","        print(f\"‚ùå Error initializing GPT-4o: {e}\")\n","        llm_classification = None\n","else:\n","    print(\"‚ùå OpenAI API key not found\")\n","    llm_classification = None\n","\n","# --- 2. Load Dataset ---\n","CSV_PATH = \"/content/drive/MyDrive/knowledge-graph-llms/data_all.csv\"\n","ENHANCED_CSV_PATH = \"/content/drive/MyDrive/knowledge-graph-llms/construction_ai_research_dataset.csv\"\n","ENHANCED_LOG_PATH = \"/content/drive/MyDrive/knowledge-graph-llms/gpt4o_classification_log.json\"\n","\n","print(f\"\\nüìÅ Loading dataset: {CSV_PATH}\")\n","\n","try:\n","    df_full = pd.read_csv(CSV_PATH)\n","    print(f\"‚úÖ Dataset loaded: {len(df_full)} jobs\")\n","except Exception as e:\n","    print(f\"‚ùå Error loading dataset: {e}\")\n","    df_full = None\n","\n","# --- 3. Enhanced Classification Function ---\n","def classify_job_research_relevance(job_title, job_text, employer, job_id):\n","    \"\"\"\n","    Use GPT-4o's superior reasoning to classify job relevance\n","    \"\"\"\n","\n","    content = f\"Job Title: {job_title}\\nEmployer: {employer}\\n\\nJob Description:\\n{job_text}\"\n","\n","    prompt = f\"\"\"You are an expert researcher studying AI, machine learning, automation, and data science adoption in the construction industry ecosystem.\n","\n","Your research question: \"How are AI/ML/automation technologies transforming roles across the construction industry?\"\n","\n","Classify this job based on its relevance to this research:\n","\n","RELEVANT includes:\n","1. Direct construction roles (may evolve with AI/automation)\n","2. Tech roles in construction companies (data scientists, AI specialists, automation engineers)\n","3. Support functions with AI potential (finance, HR, operations that may use AI/ML)\n","4. Construction ecosystem roles that could be impacted by digital transformation\n","5. Emerging hybrid roles combining traditional skills with tech\n","\n","NOT RELEVANT:\n","- Healthcare/medical roles unrelated to construction\n","- Pure retail/hospitality with no construction connection\n","- Financial services unrelated to construction industry\n","\n","Job to classify:\n","{content}\n","\n","Provide brief reasoning, then classify as RELEVANT or NOT_RELEVANT.\n","\n","Format:\n","REASONING: [Your reasoning]\n","CLASSIFICATION: RELEVANT or NOT_RELEVANT\"\"\"\n","\n","    try:\n","        response = llm_classification.invoke(prompt)\n","        response_text = response.content.strip()\n","\n","        # Parse response\n","        reasoning = \"\"\n","        classification = \"\"\n","\n","        if 'REASONING:' in response_text and 'CLASSIFICATION:' in response_text:\n","            parts = response_text.split('CLASSIFICATION:')\n","            reasoning = parts[0].replace('REASONING:', '').strip()\n","            classification = parts[1].strip().upper()\n","        else:\n","            # Fallback parsing\n","            if 'RELEVANT' in response_text.upper() and 'NOT_RELEVANT' not in response_text.upper():\n","                classification = 'RELEVANT'\n","            elif 'NOT_RELEVANT' in response_text.upper():\n","                classification = 'NOT_RELEVANT'\n","            else:\n","                classification = 'UNCLEAR'\n","            reasoning = response_text[:200] + \"...\" if len(response_text) > 200 else response_text\n","\n","        return classification, reasoning\n","\n","    except Exception as e:\n","        print(f\"‚ùå Error for {job_id}: {e}\")\n","        return \"ERROR\", str(e)\n","\n","# --- 4. Perform Classification ---\n","if df_full is not None and llm_classification is not None:\n","    print(f\"\\nüß† Starting GPT-4o enhanced classification...\")\n","    print(f\"üéØ Research Focus: Construction + AI/ML/Automation ecosystem\")\n","    print(f\"‚è±Ô∏è  Estimated time: {len(df_full) * 4 / 60:.1f} minutes\")\n","    print(f\"üí∞ Estimated cost: ${len(df_full) * 0.06:.2f}\")\n","\n","    # Initialize tracking\n","    classification_results = []\n","    relevant_jobs = []\n","    not_relevant_jobs = []\n","    error_jobs = []\n","\n","    start_time = time.time()\n","\n","    # Process each job\n","    for index, row in df_full.iterrows():\n","        job_id = row['JobID']\n","        job_title = row['Title']\n","        job_text = str(row['JobText'])\n","        employer = row.get('Employer', 'Unknown')\n","\n","        # Show progress\n","        if (index + 1) % 15 == 0:\n","            elapsed = time.time() - start_time\n","            avg_time_per_job = elapsed / (index + 1)\n","            remaining = (len(df_full) - index - 1) * avg_time_per_job\n","            print(f\"   Progress: {index + 1}/{len(df_full)} ({(index + 1)/len(df_full)*100:.1f}%) - Remaining: {remaining/60:.1f}min\")\n","\n","        # Classify with reasoning\n","        classification, reasoning = classify_job_research_relevance(job_title, job_text, employer, job_id)\n","\n","        # Store results\n","        result = {\n","            'index': index,\n","            'job_id': job_id,\n","            'title': job_title,\n","            'employer': employer,\n","            'classification': classification,\n","            'reasoning': reasoning,\n","            'text_length': len(job_text)\n","        }\n","        classification_results.append(result)\n","\n","        # Categorize\n","        if classification == \"RELEVANT\":\n","            relevant_jobs.append(row)\n","        elif classification == \"NOT_RELEVANT\":\n","            not_relevant_jobs.append(row)\n","        else:\n","            error_jobs.append(row)\n","\n","        # Rate limiting\n","        time.sleep(0.8)\n","\n","    total_time = time.time() - start_time\n","\n","    # --- 5. Results Analysis ---\n","    print(f\"\\n\" + \"=\"*70)\n","    print(f\"üß† GPT-4o CLASSIFICATION COMPLETE!\")\n","    print(f\"=\"*70)\n","    print(f\"‚è±Ô∏è  Total time: {total_time/60:.1f} minutes\")\n","    print(f\"üí∞ Cost: ${len(df_full) * 0.06:.2f}\")\n","    print(f\"\\nüìä RESEARCH RESULTS:\")\n","    print(f\"   ‚úÖ Research relevant jobs: {len(relevant_jobs)}\")\n","    print(f\"   ‚ùå Not relevant jobs: {len(not_relevant_jobs)}\")\n","    print(f\"   ‚ö†Ô∏è  Error/unclear jobs: {len(error_jobs)}\")\n","    print(f\"   üìà Research relevance rate: {len(relevant_jobs)/len(df_full)*100:.1f}%\")\n","\n","    # --- 6. Show Sample Reasoning ---\n","    print(f\"\\nüß† SAMPLE REASONING:\")\n","\n","    relevant_samples = [r for r in classification_results if r['classification'] == 'RELEVANT'][:3]\n","    print(f\"\\n‚úÖ RELEVANT JOBS:\")\n","    for i, sample in enumerate(relevant_samples, 1):\n","        print(f\"{i}. {sample['title']} at {sample['employer']}\")\n","        print(f\"   üí≠ {sample['reasoning'][:100]}...\")\n","        print()\n","\n","    not_relevant_samples = [r for r in classification_results if r['classification'] == 'NOT_RELEVANT'][:2]\n","    print(f\"‚ùå NOT RELEVANT JOBS:\")\n","    for i, sample in enumerate(not_relevant_samples, 1):\n","        print(f\"{i}. {sample['title']} at {sample['employer']}\")\n","        print(f\"   üí≠ {sample['reasoning'][:100]}...\")\n","        print()\n","\n","    # --- 7. Save Research Dataset ---\n","    if relevant_jobs:\n","        try:\n","            research_df = pd.DataFrame(relevant_jobs)\n","            research_df.to_csv(ENHANCED_CSV_PATH, index=False)\n","            print(f\"üíæ RESEARCH DATASET SAVED:\")\n","            print(f\"   üìÅ Path: {ENHANCED_CSV_PATH}\")\n","            print(f\"   üìä Size: {len(research_df)} research-relevant jobs\")\n","\n","        except Exception as e:\n","            print(f\"‚ùå Error saving dataset: {e}\")\n","\n","    # --- 8. Save Classification Log ---\n","    try:\n","        enhanced_log = {\n","            'timestamp': datetime.now().isoformat(),\n","            'model_used': 'gpt-4o',\n","            'total_jobs': len(df_full),\n","            'relevant_jobs': len(relevant_jobs),\n","            'not_relevant_jobs': len(not_relevant_jobs),\n","            'error_jobs': len(error_jobs),\n","            'processing_time_minutes': total_time / 60,\n","            'relevance_rate': len(relevant_jobs)/len(df_full)*100,\n","            'sample_results': classification_results[:20]  # First 20 with reasoning\n","        }\n","\n","        with open(ENHANCED_LOG_PATH, 'w') as f:\n","            json.dump(enhanced_log, f, indent=2)\n","\n","        print(f\"üìã Classification log saved\")\n","\n","    except Exception as e:\n","        print(f\"‚ö†Ô∏è  Error saving log: {e}\")\n","\n","    # --- 9. Ready for Next Step ---\n","    print(f\"\\n\" + \"=\"*70)\n","    print(f\"üéâ ENHANCED RESEARCH DATASET READY!\")\n","    print(f\"=\"*70)\n","    print(f\"üß† GPT-4o reasoning-based classification completed\")\n","    print(f\"üéØ {len(relevant_jobs)} jobs relevant to construction + AI/ML research\")\n","    print(f\"üìÅ Dataset ready for knowledge graph extraction\")\n","    print(f\"\\nüöÄ NEXT: Run Cell 5B to load the research dataset\")\n","    print(f\"=\"*70)\n","\n","else:\n","    print(\"‚ùå Cannot proceed. Check prerequisites.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ri8860aMR4Fp","executionInfo":{"status":"ok","timestamp":1751013016329,"user_tz":-600,"elapsed":1380191,"user":{"displayName":"M. Reza Hosseini","userId":"13449621777993109619"}},"outputId":"f218b40f-9413-45f0-87b6-87a1554fde67"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["üß† GPT-4o ENHANCED CONSTRUCTION + AI/ML ECOSYSTEM CLASSIFICATION\n","üéØ Research Focus: AI/Automation/Data Science in Construction\n","======================================================================\n","‚úÖ OpenAI API key retrieved\n","‚úÖ GPT-4o initialized for enhanced classification\n","\n","üìÅ Loading dataset: /content/drive/MyDrive/knowledge-graph-llms/data_all.csv\n","‚úÖ Dataset loaded: 300 jobs\n","\n","üß† Starting GPT-4o enhanced classification...\n","üéØ Research Focus: Construction + AI/ML/Automation ecosystem\n","‚è±Ô∏è  Estimated time: 20.0 minutes\n","üí∞ Estimated cost: $18.00\n","   Progress: 15/300 (5.0%) - Remaining: 20.0min\n","   Progress: 30/300 (10.0%) - Remaining: 19.0min\n","   Progress: 45/300 (15.0%) - Remaining: 18.7min\n","   Progress: 60/300 (20.0%) - Remaining: 17.8min\n","   Progress: 75/300 (25.0%) - Remaining: 16.6min\n","   Progress: 90/300 (30.0%) - Remaining: 15.6min\n","   Progress: 105/300 (35.0%) - Remaining: 14.5min\n","   Progress: 120/300 (40.0%) - Remaining: 13.4min\n","   Progress: 135/300 (45.0%) - Remaining: 12.3min\n","   Progress: 150/300 (50.0%) - Remaining: 11.2min\n","   Progress: 165/300 (55.0%) - Remaining: 10.1min\n","   Progress: 180/300 (60.0%) - Remaining: 9.0min\n","   Progress: 195/300 (65.0%) - Remaining: 8.0min\n","   Progress: 210/300 (70.0%) - Remaining: 6.9min\n","   Progress: 225/300 (75.0%) - Remaining: 5.8min\n","   Progress: 240/300 (80.0%) - Remaining: 4.6min\n","   Progress: 255/300 (85.0%) - Remaining: 3.5min\n","   Progress: 270/300 (90.0%) - Remaining: 2.3min\n","   Progress: 285/300 (95.0%) - Remaining: 1.2min\n","   Progress: 300/300 (100.0%) - Remaining: 0.0min\n","\n","======================================================================\n","üß† GPT-4o CLASSIFICATION COMPLETE!\n","======================================================================\n","‚è±Ô∏è  Total time: 22.9 minutes\n","üí∞ Cost: $18.00\n","\n","üìä RESEARCH RESULTS:\n","   ‚úÖ Research relevant jobs: 283\n","   ‚ùå Not relevant jobs: 10\n","   ‚ö†Ô∏è  Error/unclear jobs: 7\n","   üìà Research relevance rate: 94.3%\n","\n","üß† SAMPLE REASONING:\n","\n","‚úÖ RELEVANT JOBS:\n","1. Analytics and Insights Managers at Mirvac\n","   üí≠ The job title \"Analytics and Insights Managers\" at Mirvac is relevant to the research question as it...\n","\n","2. Automation Managers at Mirvac\n","   üí≠ The job of a Solution Engineer, Intelligent Process Automation at Mirvac is highly relevant to the r...\n","\n","3. Mechanical Service Technicians at Johnathan Thurston Academy\n","   üí≠ The role of Mechanical Service Technicians in the HVAC industry, particularly within a company like ...\n","\n","‚ùå NOT RELEVANT JOBS:\n","1. Data Scientists at Lendlease\n","   üí≠ The job description for the Senior Data Scientist position at La Trobe University does not indicate ...\n","\n","2. nan at Heb Construction\n","   üí≠ The job description for the Kaikokiri - Te Reo me nga Tikanga Maori role at HEB Construction is prim...\n","\n","üíæ RESEARCH DATASET SAVED:\n","   üìÅ Path: /content/drive/MyDrive/knowledge-graph-llms/construction_ai_research_dataset.csv\n","   üìä Size: 283 research-relevant jobs\n","üìã Classification log saved\n","\n","======================================================================\n","üéâ ENHANCED RESEARCH DATASET READY!\n","======================================================================\n","üß† GPT-4o reasoning-based classification completed\n","üéØ 283 jobs relevant to construction + AI/ML research\n","üìÅ Dataset ready for knowledge graph extraction\n","\n","üöÄ NEXT: Run Cell 5B to load the research dataset\n","======================================================================\n"]}]},{"cell_type":"code","source":["# Show all NOT RELEVANT jobs with reasoning\n","not_relevant = [r for r in classification_results if r['classification'] == 'NOT_RELEVANT']\n","not_relevant_df = pd.DataFrame(not_relevant)\n","display(not_relevant_df[['job_id', 'title', 'employer', 'reasoning']])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"wS-1kDm1hI_z","executionInfo":{"status":"ok","timestamp":1751013170277,"user_tz":-600,"elapsed":56,"user":{"displayName":"M. Reza Hosseini","userId":"13449621777993109619"}},"outputId":"72f20406-5b9a-4744-bdde-19b83886444a"},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":["                                     job_id  \\\n","0  baf042245eb8b6edf6a67344793bd3147430bf80   \n","1  d22b564d6d79b60ca19e5c931c5dd67c1d7da31b   \n","2  1a813523f31150ae46f668e4d970f0e335f4af70   \n","3  f8610fbd97a172d855732f30cbb54988963bef2e   \n","4  232d63479d9fb538034f9fb3afa03e878a98326c   \n","5  8acee2ce6061e06c0194b21e262a75da013d18ed   \n","6  e78dd956d2dd44d2484bd2f995aa228fb6c49a05   \n","7  11ca65ea546a222b24fac4fa19d347233d26cd4a   \n","8  4991352d4f812ee8f839ad10e832043962a69a6c   \n","9  218e436e768ad49a122238d45f95eb7ef8524165   \n","\n","                                  title            employer  \\\n","0                       Data Scientists           Lendlease   \n","1                                   NaN    Heb Construction   \n","2                 Patient Flow Managers       Kingswood Co.   \n","3                 Patient Flow Managers       Kingswood Co.   \n","4  Digital Marketing Account Executives        Australiance   \n","5           Customer Service Team Leads             Laminex   \n","6          Quantitative Trading Interns              Ventia   \n","7          Quantitative Trading Interns  The Infinity Group   \n","8          Quantitative Trading Interns              Clough   \n","9                Marketing Coordinators     Mayfield And Co   \n","\n","                                           reasoning  \n","0  The job description for the Senior Data Scient...  \n","1  The job description for the Kaikokiri - Te Reo...  \n","2  The job description is for a skin cancer docto...  \n","3  The job description provided is for a Patient ...  \n","4  The job of a Digital Marketing Account Executi...  \n","5  The job description for the Customer Service T...  \n","6  The job description for the Quantitative Tradi...  \n","7  The job description for the Quantitative Tradi...  \n","8  The job description for the Quantitative Tradi...  \n","9  The job description for the Marketing Coordina...  "],"text/html":["\n","  <div id=\"df-6be61ece-a3ee-4261-8dad-a21c714554f6\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>job_id</th>\n","      <th>title</th>\n","      <th>employer</th>\n","      <th>reasoning</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>baf042245eb8b6edf6a67344793bd3147430bf80</td>\n","      <td>Data Scientists</td>\n","      <td>Lendlease</td>\n","      <td>The job description for the Senior Data Scient...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>d22b564d6d79b60ca19e5c931c5dd67c1d7da31b</td>\n","      <td>NaN</td>\n","      <td>Heb Construction</td>\n","      <td>The job description for the Kaikokiri - Te Reo...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1a813523f31150ae46f668e4d970f0e335f4af70</td>\n","      <td>Patient Flow Managers</td>\n","      <td>Kingswood Co.</td>\n","      <td>The job description is for a skin cancer docto...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>f8610fbd97a172d855732f30cbb54988963bef2e</td>\n","      <td>Patient Flow Managers</td>\n","      <td>Kingswood Co.</td>\n","      <td>The job description provided is for a Patient ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>232d63479d9fb538034f9fb3afa03e878a98326c</td>\n","      <td>Digital Marketing Account Executives</td>\n","      <td>Australiance</td>\n","      <td>The job of a Digital Marketing Account Executi...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>8acee2ce6061e06c0194b21e262a75da013d18ed</td>\n","      <td>Customer Service Team Leads</td>\n","      <td>Laminex</td>\n","      <td>The job description for the Customer Service T...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>e78dd956d2dd44d2484bd2f995aa228fb6c49a05</td>\n","      <td>Quantitative Trading Interns</td>\n","      <td>Ventia</td>\n","      <td>The job description for the Quantitative Tradi...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>11ca65ea546a222b24fac4fa19d347233d26cd4a</td>\n","      <td>Quantitative Trading Interns</td>\n","      <td>The Infinity Group</td>\n","      <td>The job description for the Quantitative Tradi...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>4991352d4f812ee8f839ad10e832043962a69a6c</td>\n","      <td>Quantitative Trading Interns</td>\n","      <td>Clough</td>\n","      <td>The job description for the Quantitative Tradi...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>218e436e768ad49a122238d45f95eb7ef8524165</td>\n","      <td>Marketing Coordinators</td>\n","      <td>Mayfield And Co</td>\n","      <td>The job description for the Marketing Coordina...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6be61ece-a3ee-4261-8dad-a21c714554f6')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-6be61ece-a3ee-4261-8dad-a21c714554f6 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-6be61ece-a3ee-4261-8dad-a21c714554f6');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-c1e1d93f-cdba-41aa-9362-00f04746326f\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c1e1d93f-cdba-41aa-9362-00f04746326f')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-c1e1d93f-cdba-41aa-9362-00f04746326f button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"display(not_relevant_df[['job_id', 'title', 'employer', 'reasoning']])\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"job_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"4991352d4f812ee8f839ad10e832043962a69a6c\",\n          \"d22b564d6d79b60ca19e5c931c5dd67c1d7da31b\",\n          \"8acee2ce6061e06c0194b21e262a75da013d18ed\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Data Scientists\",\n          \"Patient Flow Managers\",\n          \"Marketing Coordinators\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"employer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"Clough\",\n          \"Heb Construction\",\n          \"Ventia\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reasoning\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"The job description for the Quantitative Trading Interns at Akuna Capital primarily focuses on financial markets, trading models, and strategies, which are not directly related to the construction industry. The role involves applying quantitative skills to trading and financial market problems, which falls under the financial services sector unrelated to construction. Although the role involves skills in mathematics, statistics, and programming, which are relevant to AI/ML, the application of these skills is specific to trading and not to the construction industry or its ecosystem.\",\n          \"The job description for the Kaikokiri - Te Reo me nga Tikanga Maori role at HEB Construction is primarily focused on cultural and language revitalization, stakeholder communications, and learning and development within the context of a construction project. While it involves significant cultural and community engagement, it does not directly relate to AI, machine learning, or automation technologies transforming roles in the construction industry. The role is more about managing cultural aspects and communications rather than integrating or being impacted by digital transformation technologies.\",\n          \"The job description for the Customer Service Team Leader at Laminex primarily focuses on leading a customer service team to enhance customer experiences. While the role involves proficiency in digital tools and technologies such as CRM systems and chatbots, it does not directly relate to AI, machine learning, or automation technologies transforming roles in the construction industry. The position is more aligned with customer service and management rather than the digital transformation of construction roles or the construction ecosystem. Therefore, it does not fit into the categories of direct construction roles, tech roles in construction companies, or support functions with AI potential within the construction industry.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}}]},{"cell_type":"code","source":["# Get reasoning for flagged jobs\n","flagged = [r for r in classification_results if r['job_id'] in job_ids]\n","for job in flagged:\n","    print(f\"JobID: {job['job_id']}\")\n","    print(f\"Title: {job['title']}\")\n","    print(f\"Employer: {job['employer']}\")\n","    print(f\"Reasoning: {job['reasoning']}\\n{'-'*50}\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8E1Lcit_hepX","executionInfo":{"status":"ok","timestamp":1751013350939,"user_tz":-600,"elapsed":29,"user":{"displayName":"M. Reza Hosseini","userId":"13449621777993109619"}},"outputId":"9a3272da-e8bb-41e1-e8d9-d20b9baf8b77"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["JobID: baf042245eb8b6edf6a67344793bd3147430bf80\n","Title: Data Scientists\n","Employer: Lendlease\n","Reasoning: The job description for the Senior Data Scientist position at La Trobe University does not indicate any direct connection to the construction industry. The role is focused on using data science skills to improve student outcomes and university strategies, which are not related to construction. Therefore, it does not fall under any of the relevant categories for the research question about AI/ML/automation technologies transforming roles in the construction industry.\n","--------------------------------------------------\n","\n","JobID: d22b564d6d79b60ca19e5c931c5dd67c1d7da31b\n","Title: nan\n","Employer: Heb Construction\n","Reasoning: The job description for the Kaikokiri - Te Reo me nga Tikanga Maori role at HEB Construction is primarily focused on cultural and language revitalization, stakeholder communications, and learning and development within the context of a construction project. While it involves significant cultural and community engagement, it does not directly relate to AI, machine learning, or automation technologies transforming roles in the construction industry. The role is more about managing cultural aspects and communications rather than integrating or being impacted by digital transformation technologies.\n","--------------------------------------------------\n","\n"]}]},{"cell_type":"code","source":["# Quick check of the Lendlease Data Scientist job\n","df_research = pd.read_csv('/content/drive/MyDrive/knowledge-graph-llms/construction_ai_research_dataset.csv')\n","df_full = pd.read_csv('/content/drive/MyDrive/knowledge-graph-llms/data_all.csv')\n","\n","# Find the Lendlease Data Scientist job that was excluded\n","lendlease_jobs = df_full[df_full['Employer'].str.contains('Lendlease', na=False, case=False)]\n","print(\"Lendlease jobs in original dataset:\")\n","for i, row in lendlease_jobs.iterrows():\n","    in_research = row['JobID'] in df_research['JobID'].values\n","    status = \"‚úÖ INCLUDED\" if in_research else \"‚ùå EXCLUDED\"\n","    print(f\"{status} {row['Title']} - {row['Employer']}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_sr1XK_qJORh","executionInfo":{"status":"ok","timestamp":1750973346512,"user_tz":-600,"elapsed":62,"user":{"displayName":"M. Reza Hosseini","userId":"13449621777993109619"}},"outputId":"948b486a-0c1d-4d34-f3dc-b9821aff465c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Lendlease jobs in original dataset:\n","‚úÖ INCLUDED Analytics Developers - Lendlease\n","‚úÖ INCLUDED Data Support Analysts - Lendlease\n","‚ùå EXCLUDED Data Scientists - Lendlease\n","‚úÖ INCLUDED nan - Lendlease\n","‚úÖ INCLUDED Workforce Managers - Lendlease\n","‚úÖ INCLUDED Paralegals - Lendlease\n","‚úÖ INCLUDED Sourcing Analysts - Lendlease\n","‚úÖ INCLUDED Automation Engineers - Lendlease\n","‚úÖ INCLUDED Digital Marketing Analysts - Lendlease\n","‚úÖ INCLUDED Marketing Performance Analysts - Lendlease\n"]}]},{"cell_type":"code","source":["# Investigate the Lendlease and \"nan\" employer issues\n","\n","import pandas as pd\n","import json\n","\n","print(\"üîç INVESTIGATING DATA QUALITY ISSUES\")\n","print(\"=\"*50)\n","\n","# Load datasets\n","df_full = pd.read_csv('/content/drive/MyDrive/knowledge-graph-llms/data_all.csv')\n","df_research = pd.read_csv('/content/drive/MyDrive/knowledge-graph-llms/construction_ai_research_dataset.csv')\n","\n","# Load classification log with reasoning\n","with open('/content/drive/MyDrive/knowledge-graph-llms/gpt4o_classification_log.json', 'r') as f:\n","    log_data = json.load(f)\n","\n","print(\"1. CHECKING LENDLEASE JOBS:\")\n","print(\"-\" * 30)\n","\n","# Find all Lendlease jobs\n","lendlease_jobs = df_full[df_full['Employer'].str.contains('Lendlease', na=False, case=False)]\n","print(f\"Total Lendlease jobs found: {len(lendlease_jobs)}\")\n","\n","for i, row in lendlease_jobs.iterrows():\n","    in_research = row['JobID'] in df_research['JobID'].values\n","    status = \"‚úÖ INCLUDED\" if in_research else \"‚ùå EXCLUDED\"\n","    print(f\"{status} {row['Title']} at {row['Employer']}\")\n","\n","    # Show first 100 chars of job description for excluded ones\n","    if not in_research:\n","        print(f\"   üìù Job description start: {str(row['JobText'])[:100]}...\")\n","\n","print(\"\\n2. CHECKING 'nan' EMPLOYER ISSUES:\")\n","print(\"-\" * 30)\n","\n","# Find jobs with nan/null employers\n","nan_employers = df_full[df_full['Employer'].isna() | (df_full['Employer'] == 'nan')]\n","print(f\"Jobs with nan/null employers: {len(nan_employers)}\")\n","\n","for i, row in nan_employers.iterrows():\n","    in_research = row['JobID'] in df_research['JobID'].values\n","    status = \"‚úÖ INCLUDED\" if in_research else \"‚ùå EXCLUDED\"\n","    employer_display = 'NULL/NaN' if pd.isna(row['Employer']) else row['Employer']\n","    print(f\"{status} {row['Title']} at {employer_display}\")\n","\n","print(\"\\n3. CHECKING NOT_RELEVANT REASONING:\")\n","print(\"-\" * 30)\n","\n","# Look at the specific reasoning for excluded jobs\n","excluded_jobs = []\n","for result in log_data.get('sample_results', []):\n","    if result['classification'] == 'NOT_RELEVANT':\n","        excluded_jobs.append(result)\n","\n","print(\"Sample NOT_RELEVANT jobs with reasoning:\")\n","for i, job in enumerate(excluded_jobs[:5], 1):\n","    print(f\"\\n{i}. {job['title']} at {job['employer']}\")\n","    print(f\"   üí≠ Reasoning: {job['reasoning']}\")\n","\n","print(\"\\n4. LOOKING FOR DATA CORRUPTION:\")\n","print(\"-\" * 30)\n","\n","# Check if there's data mixing/corruption\n","problematic_jobs = df_full[\n","    (df_full['Title'].str.contains('Data Scientist', na=False, case=False)) &\n","    (df_full['Employer'].str.contains('Lendlease', na=False, case=False))\n","]\n","\n","print(f\"Data Scientist + Lendlease jobs: {len(problematic_jobs)}\")\n","for i, row in problematic_jobs.iterrows():\n","    print(f\"JobID: {row['JobID']}\")\n","    print(f\"Title: {row['Title']}\")\n","    print(f\"Employer: {row['Employer']}\")\n","    print(f\"Job text start: {str(row['JobText'])[:200]}...\")\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"menb5IRQJn1E","executionInfo":{"status":"ok","timestamp":1750973450790,"user_tz":-600,"elapsed":100,"user":{"displayName":"M. Reza Hosseini","userId":"13449621777993109619"}},"outputId":"cd3b9837-eafb-445c-bf27-662c06117a20"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üîç INVESTIGATING DATA QUALITY ISSUES\n","==================================================\n","1. CHECKING LENDLEASE JOBS:\n","------------------------------\n","Total Lendlease jobs found: 10\n","‚úÖ INCLUDED Analytics Developers at Lendlease\n","‚úÖ INCLUDED Data Support Analysts at Lendlease\n","‚ùå EXCLUDED Data Scientists at Lendlease\n","   üìù Job description start: Sign In / Sign up\n","    Job Seeker\n","    \n","    Sign up\n","    \n","    Post your resume\n","    \n","    Employer\n","    \n"," ...\n","‚úÖ INCLUDED nan at Lendlease\n","‚úÖ INCLUDED Workforce Managers at Lendlease\n","‚úÖ INCLUDED Paralegals at Lendlease\n","‚úÖ INCLUDED Sourcing Analysts at Lendlease\n","‚úÖ INCLUDED Automation Engineers at Lendlease\n","‚úÖ INCLUDED Digital Marketing Analysts at Lendlease\n","‚úÖ INCLUDED Marketing Performance Analysts at Lendlease\n","\n","2. CHECKING 'nan' EMPLOYER ISSUES:\n","------------------------------\n","Jobs with nan/null employers: 0\n","\n","3. CHECKING NOT_RELEVANT REASONING:\n","------------------------------\n","Sample NOT_RELEVANT jobs with reasoning:\n","\n","1. Data Scientists at Lendlease\n","   üí≠ Reasoning: The job description for the Senior Data Scientist position at La Trobe University does not indicate any direct connection to the construction industry. The role is focused on using data science skills to improve student outcomes and university strategies, which are not related to construction. Therefore, it does not fall under any of the relevant categories for the research question about AI/ML/automation technologies transforming roles in the construction industry.\n","\n","4. LOOKING FOR DATA CORRUPTION:\n","------------------------------\n","Data Scientist + Lendlease jobs: 1\n","JobID: baf042245eb8b6edf6a67344793bd3147430bf80\n","Title: Data Scientists\n","Employer: Lendlease\n","Job text start: Sign In / Sign up\n","    Job Seeker\n","    \n","    Sign up\n","    \n","    Post your resume\n","    \n","    Employer\n","    \n","    Sign up\n","    \n","    Post a Job\n","    \n","    Country and language\n","    Australia\n","    Search\n","    Advanced\n"," ...\n","\n"]}]},{"cell_type":"markdown","source":["**Cell 5B: Load Research Dataset and Setup Batching**\n","Now let's create Cell 5B to load your clean 282-job dataset and set up efficient batching for graph extraction:"],"metadata":{"id":"nYZlos7-G3TK"}},{"cell_type":"code","source":["# Cell 5B: Load Research Dataset and Setup Batching\n","# Load the GPT-4o validated construction + AI/ML dataset\n","\n","import pandas as pd\n","import os\n","import json\n","from datetime import datetime\n","from langchain_core.documents import Document\n","\n","print(\"üèóÔ∏è LOADING RESEARCH DATASET AND SETTING UP BATCHING\")\n","print(\"üß† GPT-4o Validated: Construction + AI/ML Ecosystem\")\n","print(\"=\"*65)\n","\n","# --- 1. Load the Research Dataset ---\n","RESEARCH_CSV_PATH = \"/content/drive/MyDrive/knowledge-graph-llms/construction_ai_research_dataset.csv\"\n","RESEARCH_CHECKPOINT_FOLDER = \"/content/drive/MyDrive/knowledge-graph-llms/Research extraction batches\"\n","\n","print(f\"üìÅ Loading research dataset: {RESEARCH_CSV_PATH}\")\n","\n","try:\n","    df_research = pd.read_csv(RESEARCH_CSV_PATH)\n","    print(f\"‚úÖ Research dataset loaded: {len(df_research)} jobs\")\n","    print(f\"   Columns: {list(df_research.columns)}\")\n","\n","    # Check required columns\n","    required_columns = ['Title', 'JobText', 'JobID']\n","    missing_columns = [col for col in required_columns if col not in df_research.columns]\n","\n","    if missing_columns:\n","        print(f\"‚ùå Missing required columns: {missing_columns}\")\n","        df_research = None\n","    else:\n","        print(\"‚úÖ All required columns present\")\n","\n","except Exception as e:\n","    print(f\"‚ùå Error loading research dataset: {e}\")\n","    df_research = None\n","\n","# --- 2. Create Research Checkpoint Folder ---\n","try:\n","    os.makedirs(RESEARCH_CHECKPOINT_FOLDER, exist_ok=True)\n","    print(f\"‚úÖ Research checkpoint folder ready: {RESEARCH_CHECKPOINT_FOLDER}\")\n","except Exception as e:\n","    print(f\"‚ùå Error creating checkpoint folder: {e}\")\n","\n","# --- 3. Research Dataset Quality Analysis ---\n","if df_research is not None:\n","    print(f\"\\nüîç RESEARCH DATASET ANALYSIS:\")\n","\n","    # Basic quality metrics\n","    null_titles = df_research['Title'].isnull().sum()\n","    null_jobtext = df_research['JobText'].isnull().sum()\n","    null_jobids = df_research['JobID'].isnull().sum()\n","\n","    print(f\"   Data completeness:\")\n","    print(f\"   - Title: {len(df_research) - null_titles}/{len(df_research)} complete\")\n","    print(f\"   - JobText: {len(df_research) - null_jobtext}/{len(df_research)} complete\")\n","    print(f\"   - JobID: {len(df_research) - null_jobids}/{len(df_research)} complete\")\n","\n","    # Clean any remaining null JobText\n","    if null_jobtext > 0:\n","        df_clean = df_research.dropna(subset=['JobText'])\n","        print(f\"   üßπ Removed {len(df_research) - len(df_clean)} jobs with null JobText\")\n","        df_research = df_clean\n","\n","    # JobText quality analysis\n","    jobtext_lengths = df_research['JobText'].str.len()\n","    print(f\"\\n   JobText quality:\")\n","    print(f\"   - Average length: {jobtext_lengths.mean():.0f} characters\")\n","    print(f\"   - Minimum length: {jobtext_lengths.min():.0f} characters\")\n","    print(f\"   - Maximum length: {jobtext_lengths.max():.0f} characters\")\n","\n","    # Check for very short descriptions\n","    short_jobs = (jobtext_lengths < 100).sum()\n","    if short_jobs > 0:\n","        print(f\"   ‚ö†Ô∏è  {short_jobs} jobs with short descriptions (<100 chars)\")\n","\n","    # Show sample of research-relevant jobs\n","    print(f\"\\nüìù SAMPLE RESEARCH JOBS:\")\n","    sample_size = min(5, len(df_research))\n","    for i in range(sample_size):\n","        row = df_research.iloc[i]\n","        employer = row.get('Employer', 'Unknown')\n","        print(f\"   ‚Ä¢ {row['Title']} at {employer} ({len(str(row['JobText']))} chars)\")\n","\n","# --- 4. Calculate Optimal Batching ---\n","BATCH_SIZE = 20  # Smaller batches for research dataset\n","research_batches = []\n","research_batch_info = []\n","\n","if df_research is not None:\n","    print(f\"\\nüì¶ CREATING RESEARCH BATCHES:\")\n","    print(f\"   Batch size: {BATCH_SIZE} jobs per batch\")\n","\n","    total_jobs = len(df_research)\n","    num_batches = (total_jobs + BATCH_SIZE - 1) // BATCH_SIZE  # Ceiling division\n","\n","    for i in range(num_batches):\n","        start_idx = i * BATCH_SIZE\n","        end_idx = min((i + 1) * BATCH_SIZE, total_jobs)\n","\n","        batch_df = df_research.iloc[start_idx:end_idx].copy()\n","        research_batches.append(batch_df)\n","\n","        research_batch_info.append({\n","            'batch_number': i + 1,\n","            'start_index': start_idx,\n","            'end_index': end_idx - 1,\n","            'job_count': len(batch_df),\n","            'status': 'pending'\n","        })\n","\n","        print(f\"   Batch {i+1}: Jobs {start_idx+1}-{end_idx} ({len(batch_df)} jobs)\")\n","\n","    print(f\"‚úÖ Created {num_batches} research batches totaling {total_jobs} jobs\")\n","\n","# --- 5. Research Batch Processing Functions ---\n","def convert_research_batch_to_documents(batch_df, batch_number):\n","    \"\"\"Convert research batch to LangChain Documents\"\"\"\n","    documents = []\n","\n","    for i, row in batch_df.iterrows():\n","        # Enhanced content for research focus\n","        content = f\"Job Title: {row['Title']}\\n\\nJob Description:\\n{row['JobText']}\"\n","\n","        # Create metadata with research context\n","        metadata = {\n","            'job_id': row['JobID'],\n","            'title': row['Title'],\n","            'batch_number': batch_number,\n","            'original_index': i,\n","            'dataset_type': 'research_validated',\n","            'classification': 'construction_ai_ml_relevant'\n","        }\n","\n","        # Add employer and location if available\n","        for col in ['Employer', 'City', 'State']:\n","            if col in row and pd.notna(row[col]):\n","                metadata[col.lower()] = row[col]\n","\n","        # Create LangChain Document\n","        doc = Document(page_content=content, metadata=metadata)\n","        documents.append(doc)\n","\n","    return documents\n","\n","def save_research_checkpoint(batch_number, batch_info, extracted_stats=None):\n","    \"\"\"Save research batch progress\"\"\"\n","    checkpoint_data = {\n","        'timestamp': datetime.now().isoformat(),\n","        'dataset_type': 'research_construction_ai_ml',\n","        'batch_number': batch_number,\n","        'total_batches': len(batch_info),\n","        'batch_info': batch_info,\n","        'extracted_stats': extracted_stats\n","    }\n","\n","    checkpoint_file = os.path.join(RESEARCH_CHECKPOINT_FOLDER, f\"research_checkpoint_batch_{batch_number}.json\")\n","\n","    try:\n","        with open(checkpoint_file, 'w') as f:\n","            json.dump(checkpoint_data, f, indent=2)\n","        print(f\"‚úÖ Research checkpoint saved: batch_{batch_number}.json\")\n","        return True\n","    except Exception as e:\n","        print(f\"‚ùå Error saving research checkpoint: {e}\")\n","        return False\n","\n","def load_latest_research_checkpoint():\n","    \"\"\"Load latest research checkpoint\"\"\"\n","    try:\n","        checkpoint_files = [f for f in os.listdir(RESEARCH_CHECKPOINT_FOLDER)\n","                          if f.startswith('research_checkpoint_batch_')]\n","        if not checkpoint_files:\n","            return None\n","\n","        # Find latest checkpoint\n","        latest_batch = max([int(f.split('_')[-1].split('.')[0]) for f in checkpoint_files])\n","        latest_file = os.path.join(RESEARCH_CHECKPOINT_FOLDER, f\"research_checkpoint_batch_{latest_batch}.json\")\n","\n","        with open(latest_file, 'r') as f:\n","            return json.load(f)\n","    except Exception as e:\n","        print(f\"‚ö†Ô∏è  Error loading research checkpoint: {e}\")\n","        return None\n","\n","# --- 6. Check for Existing Research Progress ---\n","existing_research_checkpoint = load_latest_research_checkpoint()\n","if existing_research_checkpoint:\n","    print(f\"\\nüîÑ EXISTING RESEARCH PROGRESS FOUND:\")\n","    print(f\"   Last completed batch: {existing_research_checkpoint['batch_number']}\")\n","    print(f\"   Timestamp: {existing_research_checkpoint['timestamp']}\")\n","\n","    completed_batches = existing_research_checkpoint['batch_number']\n","    remaining_batches = len(research_batches) - completed_batches\n","    print(f\"   Remaining batches: {remaining_batches}\")\n","else:\n","    print(f\"\\nüÜï NO EXISTING RESEARCH PROGRESS - Starting fresh\")\n","\n","# --- 7. Research Processing Summary ---\n","print(\"\\n\" + \"=\"*65)\n","print(\"üìã RESEARCH BATCHING SETUP COMPLETE\")\n","print(\"=\"*65)\n","\n","if df_research is not None and research_batches:\n","    print(f\"‚úÖ Research dataset loaded: {len(df_research)} GPT-4o validated jobs\")\n","    print(f\"‚úÖ Research batches created: {len(research_batches)} batches of {BATCH_SIZE} jobs each\")\n","    print(f\"‚úÖ Research checkpoint system ready\")\n","    print(f\"‚úÖ Save location: {RESEARCH_CHECKPOINT_FOLDER}\")\n","\n","    print(f\"\\nüìä RESEARCH PROCESSING ESTIMATES:\")\n","    estimated_time_per_batch = 12.4 * BATCH_SIZE  # Based on previous performance\n","    print(f\"   Time per batch: ~{estimated_time_per_batch/60:.1f} minutes\")\n","    print(f\"   Total estimated time: ~{(estimated_time_per_batch * len(research_batches))/60:.1f} minutes\")\n","\n","    print(f\"\\nüéØ RESEARCH FOCUS:\")\n","    print(f\"   üèóÔ∏è  Construction ecosystem jobs\")\n","    print(f\"   ü§ñ AI/ML/automation relevant roles\")\n","    print(f\"   üß† GPT-4o reasoning validated\")\n","    print(f\"   üìä 94% relevance rate achieved\")\n","\n","    print(f\"\\nüöÄ NEXT STEPS:\")\n","    print(f\"   1. Run Cell 6 to process research batches\")\n","    print(f\"   2. Enhanced graph extraction with clean data\")\n","    print(f\"   3. Focus on AI/ML transformation in construction\")\n","\n","    # Store variables for Cell 6\n","    print(f\"\\nüì¶ VARIABLES READY FOR CELL 6:\")\n","    print(f\"   - research_batches: {len(research_batches)} batch DataFrames\")\n","    print(f\"   - research_batch_info: Tracking information\")\n","    print(f\"   - Research checkpoint functions: Available\")\n","\n","else:\n","    print(\"‚ùå Research setup failed. Check errors above.\")\n","\n","print(\"=\"*65)\n","\n","# Make research variables available for next cell\n","if df_research is not None and research_batches:\n","    # Create research batch status display\n","    print(f\"\\nüìä RESEARCH BATCH STATUS:\")\n","    for i, info in enumerate(research_batch_info):\n","        status_icon = \"‚è≥\" if info['status'] == 'pending' else \"‚úÖ\"\n","        print(f\"   {status_icon} Research Batch {info['batch_number']}: Jobs {info['start_index']+1}-{info['end_index']+1} ({info['job_count']} jobs)\")\n","\n","    print(f\"\\nüß† Ready for research-focused graph extraction!\")\n","    print(f\"üéØ High-quality dataset: Construction + AI/ML ecosystem\")\n","\n","# Rename variables for clarity in Cell 6\n","batches = research_batches\n","batch_info = research_batch_info\n","convert_batch_to_documents = convert_research_batch_to_documents\n","save_batch_checkpoint = save_research_checkpoint"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d1Dmr8vRKudA","executionInfo":{"status":"ok","timestamp":1751181772787,"user_tz":-600,"elapsed":2829,"user":{"displayName":"M. Reza Hosseini","userId":"13449621777993109619"}},"outputId":"690260d2-7259-4e4b-bd35-476d5616c524"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["üèóÔ∏è LOADING RESEARCH DATASET AND SETTING UP BATCHING\n","üß† GPT-4o Validated: Construction + AI/ML Ecosystem\n","=================================================================\n","üìÅ Loading research dataset: /content/drive/MyDrive/knowledge-graph-llms/construction_ai_research_dataset.csv\n","‚úÖ Research dataset loaded: 283 jobs\n","   Columns: ['Employer', 'JobID', 'City', 'State', 'Title', 'JobDate', 'ANZSIC CODE', 'ANZSCO CODE', 'LOT CODE', 'JobText', 'JobUrl']\n","‚úÖ All required columns present\n","‚úÖ Research checkpoint folder ready: /content/drive/MyDrive/knowledge-graph-llms/Research extraction batches\n","\n","üîç RESEARCH DATASET ANALYSIS:\n","   Data completeness:\n","   - Title: 271/283 complete\n","   - JobText: 283/283 complete\n","   - JobID: 283/283 complete\n","\n","   JobText quality:\n","   - Average length: 4365 characters\n","   - Minimum length: 96 characters\n","   - Maximum length: 11672 characters\n","   ‚ö†Ô∏è  1 jobs with short descriptions (<100 chars)\n","\n","üìù SAMPLE RESEARCH JOBS:\n","   ‚Ä¢ Analytics and Insights Managers at Mirvac (1604 chars)\n","   ‚Ä¢ Automation Managers at Mirvac (3979 chars)\n","   ‚Ä¢ Mechanical Service Technicians at Johnathan Thurston Academy (3973 chars)\n","   ‚Ä¢ Data Engineers at Ventia (3224 chars)\n","   ‚Ä¢ Data Engineers at Sedgman Limited (3032 chars)\n","\n","üì¶ CREATING RESEARCH BATCHES:\n","   Batch size: 20 jobs per batch\n","   Batch 1: Jobs 1-20 (20 jobs)\n","   Batch 2: Jobs 21-40 (20 jobs)\n","   Batch 3: Jobs 41-60 (20 jobs)\n","   Batch 4: Jobs 61-80 (20 jobs)\n","   Batch 5: Jobs 81-100 (20 jobs)\n","   Batch 6: Jobs 101-120 (20 jobs)\n","   Batch 7: Jobs 121-140 (20 jobs)\n","   Batch 8: Jobs 141-160 (20 jobs)\n","   Batch 9: Jobs 161-180 (20 jobs)\n","   Batch 10: Jobs 181-200 (20 jobs)\n","   Batch 11: Jobs 201-220 (20 jobs)\n","   Batch 12: Jobs 221-240 (20 jobs)\n","   Batch 13: Jobs 241-260 (20 jobs)\n","   Batch 14: Jobs 261-280 (20 jobs)\n","   Batch 15: Jobs 281-283 (3 jobs)\n","‚úÖ Created 15 research batches totaling 283 jobs\n","\n","üîÑ EXISTING RESEARCH PROGRESS FOUND:\n","   Last completed batch: 15\n","   Timestamp: 2025-06-27T00:29:55.075674\n","   Remaining batches: 0\n","\n","=================================================================\n","üìã RESEARCH BATCHING SETUP COMPLETE\n","=================================================================\n","‚úÖ Research dataset loaded: 283 GPT-4o validated jobs\n","‚úÖ Research batches created: 15 batches of 20 jobs each\n","‚úÖ Research checkpoint system ready\n","‚úÖ Save location: /content/drive/MyDrive/knowledge-graph-llms/Research extraction batches\n","\n","üìä RESEARCH PROCESSING ESTIMATES:\n","   Time per batch: ~4.1 minutes\n","   Total estimated time: ~62.0 minutes\n","\n","üéØ RESEARCH FOCUS:\n","   üèóÔ∏è  Construction ecosystem jobs\n","   ü§ñ AI/ML/automation relevant roles\n","   üß† GPT-4o reasoning validated\n","   üìä 94% relevance rate achieved\n","\n","üöÄ NEXT STEPS:\n","   1. Run Cell 6 to process research batches\n","   2. Enhanced graph extraction with clean data\n","   3. Focus on AI/ML transformation in construction\n","\n","üì¶ VARIABLES READY FOR CELL 6:\n","   - research_batches: 15 batch DataFrames\n","   - research_batch_info: Tracking information\n","   - Research checkpoint functions: Available\n","=================================================================\n","\n","üìä RESEARCH BATCH STATUS:\n","   ‚è≥ Research Batch 1: Jobs 1-20 (20 jobs)\n","   ‚è≥ Research Batch 2: Jobs 21-40 (20 jobs)\n","   ‚è≥ Research Batch 3: Jobs 41-60 (20 jobs)\n","   ‚è≥ Research Batch 4: Jobs 61-80 (20 jobs)\n","   ‚è≥ Research Batch 5: Jobs 81-100 (20 jobs)\n","   ‚è≥ Research Batch 6: Jobs 101-120 (20 jobs)\n","   ‚è≥ Research Batch 7: Jobs 121-140 (20 jobs)\n","   ‚è≥ Research Batch 8: Jobs 141-160 (20 jobs)\n","   ‚è≥ Research Batch 9: Jobs 161-180 (20 jobs)\n","   ‚è≥ Research Batch 10: Jobs 181-200 (20 jobs)\n","   ‚è≥ Research Batch 11: Jobs 201-220 (20 jobs)\n","   ‚è≥ Research Batch 12: Jobs 221-240 (20 jobs)\n","   ‚è≥ Research Batch 13: Jobs 241-260 (20 jobs)\n","   ‚è≥ Research Batch 14: Jobs 261-280 (20 jobs)\n","   ‚è≥ Research Batch 15: Jobs 281-283 (3 jobs)\n","\n","üß† Ready for research-focused graph extraction!\n","üéØ High-quality dataset: Construction + AI/ML ecosystem\n"]}]},{"cell_type":"code","source":["# Checkpoint Folder Investigation Script\n","# Read-only analysis to understand current state\n","\n","import os\n","import json\n","from datetime import datetime\n","import pandas as pd\n","\n","print(\"üîç INVESTIGATING CHECKPOINT FOLDER\")\n","print(\"üìÅ Read-only analysis of existing files\")\n","print(\"=\"*60)\n","\n","# Define checkpoint folder path\n","CHECKPOINT_FOLDER = \"/content/drive/MyDrive/knowledge-graph-llms/Research extraction batches\"\n","\n","# --- 1. Check if folder exists and list contents ---\n","print(f\"üìÇ Checkpoint folder: {CHECKPOINT_FOLDER}\")\n","\n","try:\n","    if os.path.exists(CHECKPOINT_FOLDER):\n","        print(\"‚úÖ Folder exists\")\n","\n","        # Get all files in the folder\n","        all_files = os.listdir(CHECKPOINT_FOLDER)\n","\n","        if not all_files:\n","            print(\"üì≠ Folder is empty\")\n","        else:\n","            print(f\"üìä Found {len(all_files)} files\")\n","\n","            # --- 2. Categorize files by type ---\n","            file_categories = {\n","                'checkpoint_files': [],\n","                'extraction_files': [],\n","                'summary_files': [],\n","                'other_files': []\n","            }\n","\n","            for file in all_files:\n","                if file.startswith('research_checkpoint_batch_'):\n","                    file_categories['checkpoint_files'].append(file)\n","                elif file.startswith('extracted_entities_batch_'):\n","                    file_categories['extraction_files'].append(file)\n","                elif file.startswith('extraction_summary_batch_'):\n","                    file_categories['summary_files'].append(file)\n","                else:\n","                    file_categories['other_files'].append(file)\n","\n","            # --- 3. Display file analysis ---\n","            print(f\"\\nüìã FILE ANALYSIS:\")\n","            for category, files in file_categories.items():\n","                if files:\n","                    print(f\"   {category.replace('_', ' ').title()}: {len(files)} files\")\n","                    for file in sorted(files):\n","                        file_path = os.path.join(CHECKPOINT_FOLDER, file)\n","                        file_size = os.path.getsize(file_path)\n","                        file_time = datetime.fromtimestamp(os.path.getmtime(file_path))\n","                        print(f\"     ‚Ä¢ {file} ({file_size:,} bytes, {file_time.strftime('%Y-%m-%d %H:%M')})\")\n","\n","            # --- 4. Extract batch numbers from different file types ---\n","            print(f\"\\nüî¢ BATCH NUMBER ANALYSIS:\")\n","\n","            # From checkpoint files\n","            checkpoint_batches = []\n","            for file in file_categories['checkpoint_files']:\n","                try:\n","                    batch_num = int(file.split('_')[-1].split('.')[0])\n","                    checkpoint_batches.append(batch_num)\n","                except:\n","                    pass\n","\n","            # From extraction files\n","            extraction_batches = []\n","            for file in file_categories['extraction_files']:\n","                try:\n","                    batch_num = int(file.split('_')[-1].split('.')[0])\n","                    extraction_batches.append(batch_num)\n","                except:\n","                    pass\n","\n","            # From summary files\n","            summary_batches = []\n","            for file in file_categories['summary_files']:\n","                try:\n","                    batch_num = int(file.split('_')[-1].split('.')[0])\n","                    summary_batches.append(batch_num)\n","                except:\n","                    pass\n","\n","            print(f\"   Checkpoint batches: {sorted(checkpoint_batches) if checkpoint_batches else 'None'}\")\n","            print(f\"   Extraction batches: {sorted(extraction_batches) if extraction_batches else 'None'}\")\n","            print(f\"   Summary batches: {sorted(summary_batches) if summary_batches else 'None'}\")\n","\n","            # --- 5. Sample file contents ---\n","            print(f\"\\nüìÑ SAMPLE FILE CONTENTS:\")\n","\n","            # Check latest checkpoint file\n","            if file_categories['checkpoint_files']:\n","                latest_checkpoint = max(file_categories['checkpoint_files'],\n","                                      key=lambda x: int(x.split('_')[-1].split('.')[0]))\n","                checkpoint_path = os.path.join(CHECKPOINT_FOLDER, latest_checkpoint)\n","\n","                print(f\"\\n   üìã Latest Checkpoint: {latest_checkpoint}\")\n","                try:\n","                    with open(checkpoint_path, 'r') as f:\n","                        checkpoint_data = json.load(f)\n","\n","                    print(f\"     ‚Ä¢ Timestamp: {checkpoint_data.get('timestamp', 'N/A')}\")\n","                    print(f\"     ‚Ä¢ Dataset type: {checkpoint_data.get('dataset_type', 'N/A')}\")\n","                    print(f\"     ‚Ä¢ Batch number: {checkpoint_data.get('batch_number', 'N/A')}\")\n","                    print(f\"     ‚Ä¢ Total batches: {checkpoint_data.get('total_batches', 'N/A')}\")\n","\n","                    if 'extracted_stats' in checkpoint_data:\n","                        print(f\"     ‚Ä¢ Has extraction stats: ‚úÖ\")\n","                    else:\n","                        print(f\"     ‚Ä¢ Has extraction stats: ‚ùå\")\n","\n","                except Exception as e:\n","                    print(f\"     ‚ùå Error reading checkpoint: {e}\")\n","\n","            # Check an extraction file if exists\n","            if file_categories['extraction_files']:\n","                sample_extraction = file_categories['extraction_files'][0]\n","                extraction_path = os.path.join(CHECKPOINT_FOLDER, sample_extraction)\n","\n","                print(f\"\\n   üîç Sample Extraction: {sample_extraction}\")\n","                try:\n","                    with open(extraction_path, 'r') as f:\n","                        extraction_data = json.load(f)\n","\n","                    print(f\"     ‚Ä¢ Batch number: {extraction_data.get('batch_number', 'N/A')}\")\n","                    print(f\"     ‚Ä¢ Jobs processed: {extraction_data.get('jobs_processed', 'N/A')}\")\n","                    print(f\"     ‚Ä¢ Total jobs: {extraction_data.get('total_jobs', 'N/A')}\")\n","\n","                    # Check extraction results\n","                    results = extraction_data.get('extraction_results', [])\n","                    print(f\"     ‚Ä¢ Extraction results: {len(results)} jobs\")\n","\n","                    if results:\n","                        sample_result = results[0]\n","                        entities = sample_result.get('extraction_data', {}).get('entities', [])\n","                        relationships = sample_result.get('extraction_data', {}).get('relationships', [])\n","                        print(f\"     ‚Ä¢ Sample job entities: {len(entities)}\")\n","                        print(f\"     ‚Ä¢ Sample job relationships: {len(relationships)}\")\n","\n","                    # Check insights\n","                    insights = extraction_data.get('batch_insights', {})\n","                    if insights:\n","                        print(f\"     ‚Ä¢ Total entities: {insights.get('total_entities', 0)}\")\n","                        print(f\"     ‚Ä¢ Total relationships: {insights.get('total_relationships', 0)}\")\n","                        print(f\"     ‚Ä¢ AI/ML mentions: {insights.get('ai_ml_mentions', 0)}\")\n","\n","                except Exception as e:\n","                    print(f\"     ‚ùå Error reading extraction file: {e}\")\n","\n","            # --- 6. Determine actual status ---\n","            print(f\"\\nüéØ STATUS DETERMINATION:\")\n","\n","            expected_batches = 15  # From your output\n","\n","            if extraction_batches:\n","                completed_extractions = len(extraction_batches)\n","                missing_batches = set(range(1, expected_batches + 1)) - set(extraction_batches)\n","\n","                print(f\"   ‚úÖ Completed extractions: {completed_extractions}/15\")\n","                print(f\"   üìã Completed batches: {sorted(extraction_batches)}\")\n","\n","                if missing_batches:\n","                    print(f\"   ‚è≥ Missing batches: {sorted(missing_batches)}\")\n","                else:\n","                    print(f\"   üéâ ALL BATCHES COMPLETED!\")\n","            else:\n","                print(f\"   ‚ùå No extraction files found\")\n","                print(f\"   üìù Status: Need to start extraction from beginning\")\n","\n","            # --- 7. Recommendations ---\n","            print(f\"\\nüí° RECOMMENDATIONS:\")\n","\n","            if len(extraction_batches) == expected_batches:\n","                print(f\"   üéØ All extractions complete - proceed to knowledge graph analysis\")\n","                print(f\"   üìä Skip extraction phase - move to graph construction\")\n","            elif extraction_batches:\n","                missing_count = expected_batches - len(extraction_batches)\n","                print(f\"   ‚ö° Resume extraction from batch {max(extraction_batches) + 1}\")\n","                print(f\"   üìà {missing_count} batches remaining\")\n","            else:\n","                print(f\"   üöÄ Start fresh extraction from batch 1\")\n","                print(f\"   üìù No previous extraction work found\")\n","\n","    else:\n","        print(\"‚ùå Folder does not exist\")\n","        print(\"üìù Recommendation: Create folder and start fresh extraction\")\n","\n","except Exception as e:\n","    print(f\"‚ùå Error investigating folder: {e}\")\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"üîç INVESTIGATION COMPLETE\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eSes7fpXkJHl","executionInfo":{"status":"ok","timestamp":1751148175272,"user_tz":-600,"elapsed":56,"user":{"displayName":"M. Reza Hosseini","userId":"13449621777993109619"}},"outputId":"4a3e28dd-d6c8-41c2-f034-75abff1f36cf"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["üîç INVESTIGATING CHECKPOINT FOLDER\n","üìÅ Read-only analysis of existing files\n","============================================================\n","üìÇ Checkpoint folder: /content/drive/MyDrive/knowledge-graph-llms/Research extraction batches\n","‚úÖ Folder exists\n","üìä Found 15 files\n","\n","üìã FILE ANALYSIS:\n","   Checkpoint Files: 15 files\n","     ‚Ä¢ research_checkpoint_batch_1.json (3,435 bytes, 2025-06-26 21:42)\n","     ‚Ä¢ research_checkpoint_batch_10.json (9,506 bytes, 2025-06-27 00:02)\n","     ‚Ä¢ research_checkpoint_batch_11.json (10,182 bytes, 2025-06-27 00:09)\n","     ‚Ä¢ research_checkpoint_batch_12.json (10,866 bytes, 2025-06-27 00:15)\n","     ‚Ä¢ research_checkpoint_batch_13.json (11,545 bytes, 2025-06-27 00:19)\n","     ‚Ä¢ research_checkpoint_batch_14.json (12,223 bytes, 2025-06-27 00:29)\n","     ‚Ä¢ research_checkpoint_batch_15.json (12,757 bytes, 2025-06-27 00:29)\n","     ‚Ä¢ research_checkpoint_batch_2.json (4,110 bytes, 2025-06-26 23:06)\n","     ‚Ä¢ research_checkpoint_batch_3.json (4,787 bytes, 2025-06-26 23:10)\n","     ‚Ä¢ research_checkpoint_batch_4.json (5,469 bytes, 2025-06-26 23:16)\n","     ‚Ä¢ research_checkpoint_batch_5.json (6,146 bytes, 2025-06-26 23:23)\n","     ‚Ä¢ research_checkpoint_batch_6.json (6,827 bytes, 2025-06-26 23:33)\n","     ‚Ä¢ research_checkpoint_batch_7.json (7,507 bytes, 2025-06-26 23:44)\n","     ‚Ä¢ research_checkpoint_batch_8.json (8,117 bytes, 2025-06-26 23:48)\n","     ‚Ä¢ research_checkpoint_batch_9.json (8,828 bytes, 2025-06-26 23:56)\n","\n","üî¢ BATCH NUMBER ANALYSIS:\n","   Checkpoint batches: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n","   Extraction batches: None\n","   Summary batches: None\n","\n","üìÑ SAMPLE FILE CONTENTS:\n","\n","   üìã Latest Checkpoint: research_checkpoint_batch_15.json\n","     ‚Ä¢ Timestamp: 2025-06-27T00:29:55.075674\n","     ‚Ä¢ Dataset type: research_construction_ai_ml\n","     ‚Ä¢ Batch number: 15\n","     ‚Ä¢ Total batches: 15\n","     ‚Ä¢ Has extraction stats: ‚úÖ\n","\n","üéØ STATUS DETERMINATION:\n","   ‚ùå No extraction files found\n","   üìù Status: Need to start extraction from beginning\n","\n","üí° RECOMMENDATIONS:\n","   üöÄ Start fresh extraction from batch 1\n","   üìù No previous extraction work found\n","\n","============================================================\n","üîç INVESTIGATION COMPLETE\n","============================================================\n"]}]},{"cell_type":"markdown","source":["**New Cell 6**"],"metadata":{"id":"4fhakOfnoJ9j"}},{"cell_type":"code","source":["# Cell 6: Production-Ready Research Graph Extraction\n","# Extract Roles, Tasks, Technical Skills, and Soft Skills with Production Fixes\n","\n","import time\n","import logging\n","import tempfile\n","import shutil\n","from collections import Counter\n","import json\n","import os\n","\n","print(\"üöÄ PRODUCTION-READY RESEARCH GRAPH EXTRACTION\")\n","print(\"üß† Dataset: GPT-4o Validated | Extraction: Production-Quality\")\n","print(\"üéØ Target: Roles ‚Üí Tasks ‚Üí Technical Skills ‚Üí Soft Skills\")\n","print(\"=\"*60)\n","\n","# --- 1. Setup Logging (with handler management) ---\n","def setup_logging():\n","    \"\"\"Setup structured logging for the extraction pipeline\"\"\"\n","    logger = logging.getLogger(__name__)\n","\n","    # Clear existing handlers to prevent duplicates on re-execution\n","    if logger.handlers:\n","        logger.handlers.clear()\n","\n","    # Also clear root logger handlers if they exist\n","    root_logger = logging.getLogger()\n","    if root_logger.handlers:\n","        root_logger.handlers.clear()\n","\n","    # Ensure log directory exists\n","    log_dir = RESEARCH_CHECKPOINT_FOLDER if 'RESEARCH_CHECKPOINT_FOLDER' in globals() else '/tmp'\n","    os.makedirs(log_dir, exist_ok=True)\n","\n","    log_format = '%(asctime)s - %(levelname)s - %(message)s'\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        format=log_format,\n","        handlers=[\n","            logging.StreamHandler(),  # Console output\n","            logging.FileHandler(os.path.join(log_dir, 'extraction_pipeline.log'))\n","        ]\n","    )\n","    return logger\n","\n","# --- 2. ResearchETL Class for Better Abstraction ---\n","class ResearchETL:\n","    \"\"\"Lightweight ETL class for research extraction pipeline\"\"\"\n","\n","    def __init__(self, checkpoint_folder, graph_transformer, neo4j_graph,\n","                 batches, batch_info, convert_fn, save_checkpoint_fn):\n","        self.checkpoint_folder = checkpoint_folder\n","        self.graph_transformer = graph_transformer\n","        self.neo4j_graph = neo4j_graph\n","        self.batches = batches\n","        self.batch_info = batch_info\n","        self.convert_batch_to_documents = convert_fn\n","        self.save_batch_checkpoint = save_checkpoint_fn\n","        self.logger = setup_logging()\n","        self.progress_tracker = ProgressTracker()\n","\n","        # Entity type classification lookup table (refined for edge cases)\n","        self.entity_classifiers = {\n","            'role_keywords': ['manager', 'engineer', 'analyst', 'specialist', 'coordinator', 'director', 'lead', 'head'],\n","            'task_keywords': ['task', 'responsibility', 'duty', 'manage', 'develop', 'implement', 'coordinate', 'oversee'],\n","            'soft_skill_keywords': ['communication', 'leadership', 'teamwork', 'problem solving', 'analytical thinking', 'collaboration'],\n","            'technical_skill_keywords': ['software', 'platform', 'programming', 'technology', 'system', 'framework', 'database', 'cloud']\n","        }\n","\n","    def get_next_batch_to_process(self, force_restart=False, auto_cleanup=False):\n","        \"\"\"Find next research batch that needs extraction\"\"\"\n","\n","        if force_restart:\n","            self.logger.info(\"Force restart: Starting fresh from batch 1\")\n","            if auto_cleanup:\n","                self._cleanup_extraction_files()\n","            return 1\n","\n","        # Check for actual extraction files\n","        actual_extractions = []\n","        try:\n","            extraction_files = [f for f in os.listdir(self.checkpoint_folder)\n","                              if f.startswith('extracted_entities_batch_') or\n","                                 f.startswith('extraction_results_batch_')]\n","            actual_extractions = [int(f.split('_')[-1].split('.')[0]) for f in extraction_files]\n","            actual_extractions.sort()\n","        except FileNotFoundError as e:\n","            self.logger.warning(f\"Checkpoint folder not found: {e}\")\n","            return 1\n","        except ValueError as e:\n","            self.logger.error(f\"Error parsing extraction file numbers: {e}\")\n","            return 1\n","\n","        if not actual_extractions:\n","            self.logger.info(\"No actual extractions found - starting fresh from batch 1\")\n","            return 1\n","\n","        # Find first missing batch (handles non-contiguous gaps)\n","        total_batches = len(self.batches)\n","        for i in range(1, total_batches + 1):\n","            if i not in actual_extractions:\n","                self.logger.info(f\"Found gap: processing batch {i} (completed: {actual_extractions})\")\n","                return i\n","\n","        # All batches completed\n","        return None\n","\n","    def _cleanup_extraction_files(self):\n","        \"\"\"Clean up previous extraction files\"\"\"\n","        try:\n","            extraction_files = [f for f in os.listdir(self.checkpoint_folder)\n","                              if f.startswith('extracted_entities_batch_') or\n","                                 f.startswith('extraction_results_batch_')]\n","            if extraction_files:\n","                self.logger.info(f\"Cleaning up {len(extraction_files)} previous extraction files\")\n","                for file in extraction_files:\n","                    file_path = os.path.join(self.checkpoint_folder, file)\n","                    os.remove(file_path)\n","                self.logger.info(f\"Cleaned up {len(extraction_files)} files\")\n","        except OSError as e:\n","            self.logger.error(f\"Could not clean extraction files: {e}\")\n","\n","    def classify_entity_type(self, original_type):\n","        \"\"\"Classify entity type using refined lookup table approach\"\"\"\n","        original_lower = original_type.lower()\n","\n","        # Explicit role indicators first (most specific)\n","        if any(keyword in original_lower for keyword in self.entity_classifiers['role_keywords']):\n","            return 'Role'\n","\n","        # Task indicators\n","        if any(keyword in original_lower for keyword in self.entity_classifiers['task_keywords']):\n","            return 'Task'\n","\n","        # Soft skill (check before technical to handle \"soft skill\" properly)\n","        if 'soft skill' in original_lower or any(keyword in original_lower for keyword in self.entity_classifiers['soft_skill_keywords']):\n","            return 'Soft skill'\n","\n","        # Technical skill and tool mapping\n","        if ('tool' in original_lower and 'lead' not in original_lower) or any(keyword in original_lower for keyword in self.entity_classifiers['technical_skill_keywords']):\n","            return 'Technical skill'\n","\n","        # Keep original if no clear match\n","        return original_type.title()\n","\n","    def classify_entity_type_with_context(self, node):\n","        \"\"\"Classify entity type using both type string and node ID/name context\"\"\"\n","        original_type = node.type\n","        original_lower = original_type.lower()\n","\n","        # Get node name/id for context-aware classification\n","        node_name = \"\"\n","        if hasattr(node, 'id') and node.id:\n","            node_name = str(node.id).lower()\n","        elif hasattr(node, 'name') and node.name:\n","            node_name = str(node.name).lower()\n","\n","        # Known technical entities that often get misclassified as \"Skill\"\n","        tech_indicators = [\n","            # Programming languages\n","            'python', 'java', 'javascript', 'c#', 'c++', '.net', 'sql', 'r', 'scala', 'go', 'rust',\n","            # Frameworks/Libraries\n","            'react', 'angular', 'vue', 'django', 'flask', 'spring', 'tensorflow', 'pytorch',\n","            # Software/Tools\n","            'excel', 'powerbi', 'tableau', 'autocad', 'revit', 'solidworks', 'matlab', 'sap',\n","            # Cloud/Infrastructure\n","            'aws', 'azure', 'gcp', 'docker', 'kubernetes', 'jenkins', 'git', 'linux',\n","            # Databases\n","            'mysql', 'postgresql', 'mongodb', 'oracle', 'redis', 'elasticsearch'\n","        ]\n","\n","        # Explicit role indicators first (most specific)\n","        if any(keyword in original_lower for keyword in self.entity_classifiers['role_keywords']):\n","            return 'Role'\n","\n","        # Task indicators\n","        if any(keyword in original_lower for keyword in self.entity_classifiers['task_keywords']):\n","            return 'Task'\n","\n","        # Handle generic \"Skill\" with context from node name\n","        if original_lower == 'skill':\n","            # Check if node name suggests technical skill\n","            if any(tech in node_name for tech in tech_indicators):\n","                return 'Technical skill'\n","            # Check if node name suggests soft skill\n","            elif any(keyword in node_name for keyword in self.entity_classifiers['soft_skill_keywords']):\n","                return 'Soft skill'\n","            # Default generic skills to Technical (most programming/tools end up here)\n","            else:\n","                return 'Technical skill'\n","\n","        # Soft skill (check before technical to handle \"soft skill\" properly)\n","        if 'soft skill' in original_lower or any(keyword in original_lower for keyword in self.entity_classifiers['soft_skill_keywords']):\n","            return 'Soft skill'\n","\n","        # Technical skill and tool mapping\n","        if ('tool' in original_lower and 'lead' not in original_lower) or any(keyword in original_lower for keyword in self.entity_classifiers['technical_skill_keywords']):\n","            return 'Technical skill'\n","\n","        # Generic skill fallback (but avoid role titles with \"skill\" in them)\n","        if 'skill' in original_lower and not any(role_kw in original_lower for role_kw in self.entity_classifiers['role_keywords']):\n","            return 'Technical skill'  # Changed default from Soft to Technical\n","\n","        # Keep original if no clear match\n","        return original_type.title()\n","\n","    def process_and_clean_entities(self, batch_graph_documents):\n","        \"\"\"Clean and standardize extracted entities using context-aware classification\"\"\"\n","        cleaned_documents = []\n","\n","        for graph_doc in batch_graph_documents:\n","            cleaned_nodes = []\n","            for node in graph_doc.nodes:\n","                node.type = self.classify_entity_type_with_context(node)\n","                cleaned_nodes.append(node)\n","\n","            graph_doc.nodes = cleaned_nodes\n","            cleaned_documents.append(graph_doc)\n","\n","        return cleaned_documents\n","\n","    def count_research_entities(self, node_types_counter):\n","        \"\"\"Count research entity types from Counter object\"\"\"\n","        entity_counts = {\n","            'roles': node_types_counter.get('Role', 0),\n","            'tasks': node_types_counter.get('Task', 0),\n","            'technical_skills': node_types_counter.get('Technical skill', 0),\n","            'soft_skills': node_types_counter.get('Soft skill', 0),\n","            'other': sum(count for node_type, count in node_types_counter.items()\n","                        if node_type not in ['Role', 'Task', 'Technical skill', 'Soft skill'])\n","        }\n","\n","        target_types = {'Role', 'Task', 'Technical skill', 'Soft skill'}\n","        return entity_counts, target_types\n","\n","    def save_results_atomically(self, batch_number, extraction_results):\n","        \"\"\"Save results with atomic write for resilience\"\"\"\n","        # Ensure checkpoint directory exists\n","        os.makedirs(self.checkpoint_folder, exist_ok=True)\n","\n","        results_file = os.path.join(self.checkpoint_folder, f\"extraction_results_batch_{batch_number}.json\")\n","\n","        try:\n","            # Write to temporary file first\n","            with tempfile.NamedTemporaryFile(mode='w', delete=False,\n","                                           dir=self.checkpoint_folder,\n","                                           prefix=f'tmp_batch_{batch_number}_') as tmp_file:\n","                json.dump(extraction_results, tmp_file, indent=2)\n","                tmp_path = tmp_file.name\n","\n","            # Atomic move to final location\n","            shutil.move(tmp_path, results_file)\n","            self.logger.info(f\"Extraction results saved atomically: extraction_results_batch_{batch_number}.json\")\n","            return True\n","\n","        except (OSError, IOError) as e:\n","            self.logger.error(f\"Error saving extraction results: {e}\")\n","            # Clean up temp file if it exists\n","            try:\n","                if 'tmp_path' in locals():\n","                    os.unlink(tmp_path)\n","            except OSError:\n","                pass\n","            return False\n","\n","    def process_batch(self, batch_number):\n","        \"\"\"Process a single batch with comprehensive error handling\"\"\"\n","        self.logger.info(f\"Starting processing of batch {batch_number}\")\n","\n","        try:\n","            batch_df = self.batches[batch_number - 1]\n","\n","            # Convert to documents\n","            self.logger.info(f\"Converting batch {batch_number} to documents\")\n","            batch_documents = self.convert_batch_to_documents(batch_df, batch_number)\n","\n","            # Extract using graph transformer\n","            self.logger.info(f\"Extracting entities from batch {batch_number}\")\n","            start_time = time.time()\n","\n","            batch_graph_documents = self.graph_transformer.convert_to_graph_documents(batch_documents)\n","\n","            # Clean and standardize\n","            cleaned_graph_documents = self.process_and_clean_entities(batch_graph_documents)\n","\n","            processing_time = time.time() - start_time\n","            self.progress_tracker.add_time(processing_time)\n","\n","            # Analyze results (with safe attribute access)\n","            batch_nodes = []\n","            batch_relationships = []\n","\n","            for graph_doc in cleaned_graph_documents:\n","                for node in graph_doc.nodes:\n","                    if hasattr(node, 'id') and hasattr(node, 'type'):\n","                        batch_nodes.append(node)\n","                    else:\n","                        self.logger.warning(f\"Node missing id or type attributes: {node}\")\n","\n","                for rel in graph_doc.relationships:\n","                    if hasattr(rel, 'type'):\n","                        batch_relationships.append(rel)\n","                    else:\n","                        self.logger.warning(f\"Relationship missing type attribute: {rel}\")\n","\n","            node_types = Counter([node.type for node in batch_nodes])\n","            relationship_types = Counter([rel.type for rel in batch_relationships])\n","\n","            # Add to Neo4j\n","            self.logger.info(f\"Adding batch {batch_number} to Neo4j\")\n","            self.neo4j_graph.add_graph_documents(cleaned_graph_documents)\n","\n","            # Prepare results\n","            research_entity_counts, target_types = self.count_research_entities(node_types)\n","\n","            target_types_found = set(node_types.keys()) & target_types\n","            consistency_score = len(target_types_found) / len(target_types)\n","\n","            extraction_results = {\n","                'batch_number': batch_number,\n","                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n","                'processing_time': processing_time,\n","                'documents_processed': len(batch_documents),\n","                'total_entities': len(batch_nodes),\n","                'total_relationships': len(batch_relationships),\n","                'entity_types': dict(node_types),\n","                'relationship_types': dict(relationship_types),\n","                'research_entity_counts': research_entity_counts,\n","                'quality_metrics': {\n","                    'entity_type_consistency': consistency_score,\n","                    'neo4j_insertion_success': True,\n","                    'tools_successfully_merged': node_types.get('Tool', 0) == 0\n","                },\n","                'sample_entities': {\n","                    'roles': list(set([str(n.id) for n in batch_nodes if n.type == 'Role']))[:5],\n","                    'tasks': list(set([str(n.id) for n in batch_nodes if n.type == 'Task']))[:5],\n","                    'technical_skills': list(set([str(n.id) for n in batch_nodes if n.type == 'Technical skill']))[:5],\n","                    'soft_skills': list(set([str(n.id) for n in batch_nodes if n.type == 'Soft skill']))[:5]\n","                }\n","            }\n","\n","            # Save results atomically\n","            if self.save_results_atomically(batch_number, extraction_results):\n","                # Update batch info only if save succeeded\n","                self.batch_info[batch_number - 1]['status'] = 'completed'\n","                self.batch_info[batch_number - 1]['processing_time'] = processing_time\n","\n","                # Save checkpoint\n","                try:\n","                    self.save_batch_checkpoint(batch_number, self.batch_info, extraction_results)\n","                except Exception as e:\n","                    self.logger.error(f\"Error saving checkpoint: {e}\")\n","                    # Don't fail the batch for checkpoint issues\n","\n","                self.logger.info(f\"Batch {batch_number} completed successfully\")\n","                return extraction_results\n","            else:\n","                raise RuntimeError(\"Failed to save extraction results\")\n","\n","        except Exception as e:\n","            self.logger.error(f\"Error processing batch {batch_number}: {e}\")\n","            raise\n","\n","# --- 3. Progress Tracking with Moving Average ---\n","class ProgressTracker:\n","    def __init__(self):\n","        self.processing_times = []\n","        self.max_history = 5\n","\n","    def add_time(self, time_seconds):\n","        self.processing_times.append(time_seconds)\n","        if len(self.processing_times) > self.max_history:\n","            self.processing_times.pop(0)\n","\n","    def estimate_remaining_time(self, remaining_batches):\n","        if not self.processing_times:\n","            return 0\n","        avg_time = sum(self.processing_times) / len(self.processing_times)\n","        return (avg_time * remaining_batches) / 60\n","\n","# --- 4. Prerequisites Check ---\n","def check_prerequisites():\n","    \"\"\"Check all required variables and return missing ones\"\"\"\n","    required_vars = {\n","        'batches': 'batches (run Cell 5B)',\n","        'graph_transformer': 'graph_transformer (run Cell 2)',\n","        'graph': 'Neo4j graph connection (run Cell 2)',\n","        'convert_batch_to_documents': 'convert_batch_to_documents function (run Cell 5B)',\n","        'save_batch_checkpoint': 'save_batch_checkpoint function (run Cell 5B)',\n","        'batch_info': 'batch_info variable (run Cell 5B)',\n","        'RESEARCH_CHECKPOINT_FOLDER': 'RESEARCH_CHECKPOINT_FOLDER (run Cell 5B)'\n","    }\n","\n","    missing = []\n","    for var_name, description in required_vars.items():\n","        if var_name not in globals() or globals()[var_name] is None:\n","            missing.append(description)\n","\n","    return missing\n","\n","# --- 5. Main Execution ---\n","\n","# Configuration\n","FORCE_RESTART = False  # Set to True to start from batch 1\n","AUTO_CLEANUP = True    # Automatically clean files on restart (no prompt)\n","\n","# Check prerequisites\n","missing_prereqs = check_prerequisites()\n","if missing_prereqs:\n","    print(\"‚ùå Missing prerequisites:\")\n","    for prereq in missing_prereqs:\n","        print(f\"   - {prereq}\")\n","    print(\"‚ö†Ô∏è  Cannot proceed. Please run previous cells first.\")\n","    raise RuntimeError(f\"Missing prerequisites: {missing_prereqs}\")\n","\n","print(\"‚úÖ All prerequisites available\")\n","\n","# Initialize ETL pipeline\n","try:\n","    etl = ResearchETL(\n","        checkpoint_folder=RESEARCH_CHECKPOINT_FOLDER,\n","        graph_transformer=graph_transformer,\n","        neo4j_graph=graph,\n","        batches=batches,\n","        batch_info=batch_info,\n","        convert_fn=convert_batch_to_documents,\n","        save_checkpoint_fn=save_batch_checkpoint\n","    )\n","    print(\"‚úÖ ETL pipeline initialized\")\n","except Exception as e:\n","    print(f\"‚ùå Failed to initialize ETL pipeline: {e}\")\n","    raise\n","\n","# Determine current batch\n","current_batch_num = etl.get_next_batch_to_process(\n","    force_restart=FORCE_RESTART,\n","    auto_cleanup=AUTO_CLEANUP\n",")\n","\n","if current_batch_num is None:\n","    print(\"üéâ ALL RESEARCH BATCHES COMPLETED!\")\n","    print(f\"‚úÖ All {len(batches)} research batches processed successfully\")\n","    print(\"üìä Ready for role and skills analysis!\")\n","\n","    # Show final statistics\n","    try:\n","        target_entity_stats_query = \"\"\"\n","        MATCH (n)\n","        WHERE labels(n)[0] IN ['Role', 'Task', 'Technical skill', 'Soft skill']\n","        RETURN labels(n)[0] as entity_type, count(n) as count\n","        ORDER BY\n","            CASE labels(n)[0]\n","                WHEN 'Role' THEN 1\n","                WHEN 'Task' THEN 2\n","                WHEN 'Technical skill' THEN 3\n","                WHEN 'Soft skill' THEN 4\n","            END\n","        \"\"\"\n","        target_stats = graph.query(target_entity_stats_query)\n","        rel_count = graph.query(\"MATCH ()-[r]-() RETURN count(r) as count\")[0]['count']\n","\n","        print(f\"\\nüìä FINAL RESEARCH EXTRACTION RESULTS:\")\n","        total_target_entities = sum([stat['count'] for stat in target_stats])\n","        print(f\"  Target Research Entities: {total_target_entities}\")\n","        for stat in target_stats:\n","            print(f\"    - {stat['entity_type']}: {stat['count']}\")\n","        print(f\"  Total Relationships: {rel_count}\")\n","\n","        jobs_processed = sum([len(batch) for batch in batches])\n","        print(f\"\\nüéØ RESEARCH PRODUCTIVITY:\")\n","        print(f\"  üìã Jobs processed: {jobs_processed}\")\n","        print(f\"  üèóÔ∏è  Target entities per job: {total_target_entities/jobs_processed:.1f}\")\n","        print(f\"  üîó Relationships per job: {rel_count/jobs_processed:.1f}\")\n","\n","        print(f\"\\nüöÄ RESEARCH ANALYSIS READY:\")\n","        print(f\"  1. Role emergence analysis\")\n","        print(f\"  2. Skills requirement mapping\")\n","        print(f\"  3. Construction + AI/ML transformation insights\")\n","\n","    except Exception as e:\n","        etl.logger.error(f\"Error getting final stats: {e}\")\n","        print(f\"‚ö†Ô∏è  Error getting final stats: {e}\")\n","\n","else:\n","    current_batch_df = batches[current_batch_num - 1]\n","\n","    print(f\"üì¶ PROCESSING RESEARCH BATCH {current_batch_num}/{len(batches)}\")\n","    print(f\"   Jobs in this batch: {len(current_batch_df)}\")\n","    print(f\"   üéØ Target: Role, Task, Technical skill, Soft skill\")\n","    print(f\"   üîß Quality: Production-ready with atomic saves\")\n","\n","    # Show sample jobs\n","    print(f\"\\nüìù SAMPLE JOBS IN THIS BATCH:\")\n","    for i in range(min(3, len(current_batch_df))):\n","        row = current_batch_df.iloc[i]\n","        employer = row.get('Employer', 'Unknown')\n","        title_preview = row['Title'][:60] + \"...\" if len(row['Title']) > 60 else row['Title']\n","        print(f\"   ‚Ä¢ {title_preview} @ {employer}\")\n","\n","    # Process the batch\n","    try:\n","        print(f\"\\nü§ñ Processing batch {current_batch_num}...\")\n","        results = etl.process_batch(current_batch_num)\n","\n","        # Display results\n","        print(f\"‚úÖ Batch {current_batch_num} processed in {results['processing_time']:.1f} seconds\")\n","        print(f\"\\nüìà EXTRACTION RESULTS:\")\n","        print(f\"  üìä Total entities: {results['total_entities']}\")\n","\n","        for entity_type in ['Role', 'Task', 'Technical skill', 'Soft skill']:\n","            count = results['entity_types'].get(entity_type, 0)\n","            print(f\"    - {entity_type}: {count}\")\n","\n","        print(f\"  üîó Total relationships: {results['total_relationships']}\")\n","\n","        # Show samples\n","        print(f\"\\nüìù SAMPLE ENTITIES:\")\n","        for category, samples in results['sample_entities'].items():\n","            if samples:\n","                print(f\"  {category}: {', '.join(samples)}\")\n","\n","        # Progress summary (accurate completion counting)\n","        completed_batches = sum(1 for b in etl.batch_info if b.get('status') == 'completed')\n","        remaining_batches = len(batches) - completed_batches\n","\n","        print(f\"\\n\" + \"=\"*60)\n","        print(f\"üéØ BATCH {current_batch_num} COMPLETE!\")\n","        print(f\"=\"*60)\n","        print(f\"‚úÖ Progress: {completed_batches}/{len(batches)} batches\")\n","        print(f\"‚è≥ Remaining: {remaining_batches} batches\")\n","\n","        if remaining_batches > 0:\n","            estimated_time = etl.progress_tracker.estimate_remaining_time(remaining_batches)\n","            print(f\"‚è±Ô∏è  Estimated remaining: {estimated_time:.1f} minutes\")\n","            print(f\"\\nüöÄ NEXT: Run this cell again for batch {current_batch_num + 1}\")\n","        else:\n","            print(f\"üéâ ALL BATCHES COMPLETE!\")\n","\n","        print(f\"=\"*60)\n","\n","    except Exception as e:\n","        etl.logger.error(f\"Batch {current_batch_num} failed: {e}\")\n","        print(f\"‚ùå BATCH {current_batch_num} FAILED!\")\n","        print(f\"‚ö†Ô∏è  Error: {e}\")\n","        print(f\"üîÑ Fix the issue and run this cell again to retry\")\n","        raise"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zpas4jDzoNkS","executionInfo":{"status":"ok","timestamp":1751185322991,"user_tz":-600,"elapsed":31381,"user":{"displayName":"M. Reza Hosseini","userId":"13449621777993109619"}},"outputId":"4324db8b-3248-4fb8-ef40-6da65f4fc9eb"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stderr","text":["2025-06-29 08:21:31,729 - INFO - Found gap: processing batch 15 (completed: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14])\n","2025-06-29 08:21:31,737 - INFO - Starting processing of batch 15\n","2025-06-29 08:21:31,739 - INFO - Converting batch 15 to documents\n","2025-06-29 08:21:31,743 - INFO - Extracting entities from batch 15\n"]},{"output_type":"stream","name":"stdout","text":["üöÄ PRODUCTION-READY RESEARCH GRAPH EXTRACTION\n","üß† Dataset: GPT-4o Validated | Extraction: Production-Quality\n","üéØ Target: Roles ‚Üí Tasks ‚Üí Technical Skills ‚Üí Soft Skills\n","============================================================\n","‚úÖ All prerequisites available\n","‚úÖ ETL pipeline initialized\n","üì¶ PROCESSING RESEARCH BATCH 15/15\n","   Jobs in this batch: 3\n","   üéØ Target: Role, Task, Technical skill, Soft skill\n","   üîß Quality: Production-ready with atomic saves\n","\n","üìù SAMPLE JOBS IN THIS BATCH:\n","   ‚Ä¢ Electricians @ Ventia\n","   ‚Ä¢ Directors of Business Intelligence @ V-Line Corporation\n","   ‚Ä¢ BIM Managers @ Metricon Homes\n","\n","ü§ñ Processing batch 15...\n"]},{"output_type":"stream","name":"stderr","text":["2025-06-29 08:21:41,100 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2025-06-29 08:21:49,878 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2025-06-29 08:22:00,563 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2025-06-29 08:22:00,586 - INFO - Adding batch 15 to Neo4j\n","2025-06-29 08:22:02,998 - INFO - Extraction results saved atomically: extraction_results_batch_15.json\n","2025-06-29 08:22:03,019 - INFO - Batch 15 completed successfully\n"]},{"output_type":"stream","name":"stdout","text":["‚úÖ Research checkpoint saved: batch_15.json\n","‚úÖ Batch 15 processed in 28.8 seconds\n","\n","üìà EXTRACTION RESULTS:\n","  üìä Total entities: 42\n","    - Role: 3\n","    - Task: 15\n","    - Technical skill: 17\n","    - Soft skill: 7\n","  üîó Total relationships: 50\n","\n","üìù SAMPLE ENTITIES:\n","  roles: Director Of Data Management And Business Intelligence, Electrician, Bim Manager\n","  tasks: System Implementation, Radio & Optic Fibre Technologies Fault Diagnostics And Rectification, Data Strategy, Content Library Management, Emergency Power Plants\n","  technical_skills: Bim Systems, Confined Space, Qld Electrical Licence, Maximo, Business Intelligence Systems\n","  soft_skills: Communication, Problem-Solving, Leadership, Project Management\n","\n","============================================================\n","üéØ BATCH 15 COMPLETE!\n","============================================================\n","‚úÖ Progress: 15/15 batches\n","‚è≥ Remaining: 0 batches\n","üéâ ALL BATCHES COMPLETE!\n","============================================================\n"]}]},{"cell_type":"code","source":["import pandas as pd   # already imported earlier\n","\n","# Normalise every batch dataframe\n","for df in batches:\n","    df['Title']    = df['Title'].fillna('Untitled role').astype(str)\n","    df['Employer'] = df['Employer'].fillna('Unknown').astype(str)\n"],"metadata":{"id":"-KYMvepEr1mv","executionInfo":{"status":"ok","timestamp":1751183746593,"user_tz":-600,"elapsed":24,"user":{"displayName":"M. Reza Hosseini","userId":"13449621777993109619"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["**Cell 6: Interactive Research Graph Extraction**\n"],"metadata":{"id":"_wFkfHisHqY9"}},{"cell_type":"code","source":["# Cell 6: Interactive Research Graph Extraction\n","# Process GPT-4o validated construction + AI/ML jobs with GPT-4o-mini\n","\n","import time\n","from collections import Counter\n","import json\n","\n","print(\"üöÄ INTERACTIVE RESEARCH GRAPH EXTRACTION\")\n","print(\"üß† Dataset: GPT-4o Validated | Extraction: GPT-4o-mini\")\n","print(\"üéØ Focus: Construction + AI/ML Ecosystem\")\n","print(\"=\"*60)\n","\n","# --- 1. Determine Next Research Batch ---\n","def get_next_research_batch_to_process():\n","    \"\"\"Find next research batch to process\"\"\"\n","    completed_batches = []\n","\n","    try:\n","        checkpoint_files = [f for f in os.listdir(RESEARCH_CHECKPOINT_FOLDER)\n","                          if f.startswith('research_checkpoint_batch_')]\n","        completed_batches = [int(f.split('_')[-1].split('.')[0]) for f in checkpoint_files]\n","        completed_batches.sort()\n","    except:\n","        completed_batches = []\n","\n","    if not completed_batches:\n","        return 1\n","\n","    next_batch = max(completed_batches) + 1\n","    return next_batch if next_batch <= len(batches) else None\n","\n","# --- 2. Check Prerequisites ---\n","prerequisites_ok = True\n","\n","if 'batches' not in locals() or not batches:\n","    print(\"‚ùå No research batches found. Please run Cell 5B first.\")\n","    prerequisites_ok = False\n","\n","if 'graph_transformer' not in locals() or graph_transformer is None:\n","    print(\"‚ùå Graph transformer not available. Please run Cell 2 first.\")\n","    prerequisites_ok = False\n","\n","if 'graph' not in locals() or graph is None:\n","    print(\"‚ùå Neo4j graph not available. Please run Cell 2 first.\")\n","    prerequisites_ok = False\n","\n","if not prerequisites_ok:\n","    print(\"‚ö†Ô∏è  Cannot proceed. Please run previous cells first.\")\n","else:\n","    # --- 3. Determine Current Research Batch ---\n","    current_batch_num = get_next_research_batch_to_process()\n","\n","    if current_batch_num is None:\n","        print(\"üéâ ALL RESEARCH BATCHES COMPLETED!\")\n","        print(\"‚úÖ All 15 research batches processed successfully\")\n","        print(\"üß† GPT-4o validated dataset ‚Üí Clean knowledge graph\")\n","\n","        # Show final research graph statistics\n","        try:\n","            final_stats_query = \"\"\"\n","            MATCH (n)\n","            RETURN labels(n)[0] as node_type, count(n) as count\n","            ORDER BY count DESC\n","            \"\"\"\n","            final_stats = graph.query(final_stats_query)\n","            rel_count = graph.query(\"MATCH ()-[r]-() RETURN count(r) as count\")[0]['count']\n","\n","            print(f\"\\nüìä FINAL RESEARCH GRAPH STATISTICS:\")\n","            total_nodes = sum([stat['count'] for stat in final_stats])\n","            print(f\"  Total Nodes: {total_nodes}\")\n","            for stat in final_stats:\n","                print(f\"    - {stat['node_type']}: {stat['count']}\")\n","            print(f\"  Total Relationships: {rel_count}\")\n","\n","            # Calculate research metrics\n","            print(f\"\\nüéØ RESEARCH METRICS:\")\n","            print(f\"  üìä Jobs processed: 282 (GPT-4o validated)\")\n","            print(f\"  üèóÔ∏è  Entities per job: {total_nodes/282:.1f}\")\n","            print(f\"  üîó Relationships per job: {rel_count/282:.1f}\")\n","            print(f\"  üß† Dataset quality: Research-grade\")\n","\n","        except Exception as e:\n","            print(f\"‚ö†Ô∏è  Error getting final stats: {e}\")\n","\n","    else:\n","        current_batch_df = batches[current_batch_num - 1]\n","\n","        print(f\"üì¶ PROCESSING RESEARCH BATCH {current_batch_num}/15\")\n","        print(f\"   Jobs in this batch: {len(current_batch_df)}\")\n","        print(f\"   Job range: {(current_batch_num-1)*20 + 1} to {min(current_batch_num*20, 282)}\")\n","        print(f\"   üß† Dataset: GPT-4o validated construction + AI/ML\")\n","\n","        # Show sample research jobs in this batch\n","        print(f\"\\nüìù SAMPLE RESEARCH JOBS IN THIS BATCH:\")\n","        for i in range(min(3, len(current_batch_df))):\n","            row = current_batch_df.iloc[i]\n","            employer = row.get('Employer', 'Unknown')\n","            print(f\"   ‚Ä¢ {row['Title']} at {employer} ({len(str(row['JobText']))} chars)\")\n","\n","        # --- 4. Convert Research Batch to Documents ---\n","        try:\n","            print(f\"\\nüîÑ Converting research batch to documents...\")\n","            batch_documents = convert_batch_to_documents(current_batch_df, current_batch_num)\n","            print(f\"‚úÖ Created {len(batch_documents)} research documents\")\n","\n","        except Exception as e:\n","            print(f\"‚ùå Error converting batch to documents: {e}\")\n","            batch_documents = []\n","\n","        # --- 5. Process with GPT-4o-mini Graph Transformer ---\n","        if batch_documents:\n","            try:\n","                print(f\"\\nü§ñ Processing with GPT-4o-mini (research-focused extraction)...\")\n","                start_time = time.time()\n","\n","                # Extract graph documents using the transformer\n","                batch_graph_documents = graph_transformer.convert_to_graph_documents(batch_documents)\n","\n","                processing_time = time.time() - start_time\n","                print(f\"‚úÖ Research batch processed in {processing_time:.1f} seconds\")\n","                print(f\"üìä Generated {len(batch_graph_documents)} graph documents\")\n","\n","                # --- 6. Analyze Research Extraction Results ---\n","                batch_nodes = []\n","                batch_relationships = []\n","\n","                for graph_doc in batch_graph_documents:\n","                    batch_nodes.extend(graph_doc.nodes)\n","                    batch_relationships.extend(graph_doc.relationships)\n","\n","                # Count by type\n","                node_types = Counter([node.type for node in batch_nodes])\n","                relationship_types = Counter([rel.type for rel in batch_relationships])\n","\n","                print(f\"\\nüìà RESEARCH BATCH {current_batch_num} RESULTS:\")\n","                print(f\"  Research entities extracted: {len(batch_nodes)}\")\n","                for node_type, count in node_types.most_common():\n","                    print(f\"    - {node_type}: {count}\")\n","\n","                print(f\"  Research relationships extracted: {len(batch_relationships)}\")\n","                for rel_type, count in relationship_types.most_common():\n","                    print(f\"    - {rel_type}: {count}\")\n","\n","                # Show sample entities for research validation\n","                print(f\"\\nüìù SAMPLE RESEARCH ENTITIES:\")\n","                for node_type in ['Role', 'Technical skill', 'Soft skill', 'Tool']:\n","                    sample_nodes = [node.id for node in batch_nodes if node.type == node_type][:3]\n","                    if sample_nodes:\n","                        print(f\"  {node_type}: {', '.join(sample_nodes)}\")\n","\n","                # --- 7. Add to Neo4j Research Graph ---\n","                try:\n","                    print(f\"\\nüíæ Adding to research knowledge graph...\")\n","                    graph.add_graph_documents(batch_graph_documents)\n","                    print(f\"‚úÖ Research batch {current_batch_num} successfully added to Neo4j\")\n","\n","                    # Get updated research graph statistics\n","                    db_stats_query = \"\"\"\n","                    MATCH (n)\n","                    RETURN labels(n)[0] as node_type, count(n) as count\n","                    ORDER BY count DESC\n","                    \"\"\"\n","                    db_stats = graph.query(db_stats_query)\n","                    rel_count = graph.query(\"MATCH ()-[r]-() RETURN count(r) as count\")[0]['count']\n","\n","                    print(f\"\\nüìä UPDATED RESEARCH GRAPH:\")\n","                    total_nodes = sum([stat['count'] for stat in db_stats])\n","                    print(f\"  Total Nodes: {total_nodes}\")\n","                    for stat in db_stats[:5]:  # Show top 5 node types\n","                        print(f\"    - {stat['node_type']}: {stat['count']}\")\n","                    print(f\"  Total Relationships: {rel_count}\")\n","\n","                except Exception as e:\n","                    print(f\"‚ùå Error adding to Neo4j: {e}\")\n","                    batch_graph_documents = []\n","\n","                # --- 8. Save Research Checkpoint ---\n","                extraction_stats = {\n","                    'processing_time': processing_time,\n","                    'documents_processed': len(batch_documents),\n","                    'entities_extracted': len(batch_nodes),\n","                    'relationships_extracted': len(batch_relationships),\n","                    'node_types': dict(node_types),\n","                    'relationship_types': dict(relationship_types),\n","                    'dataset_type': 'research_validated'\n","                }\n","\n","                # Update batch info\n","                batch_info[current_batch_num - 1]['status'] = 'completed'\n","                batch_info[current_batch_num - 1]['processing_time'] = processing_time\n","                batch_info[current_batch_num - 1]['extraction_stats'] = extraction_stats\n","\n","                checkpoint_saved = save_batch_checkpoint(current_batch_num, batch_info, extraction_stats)\n","\n","                # --- 9. Research Progress Summary ---\n","                completed_batches = current_batch_num\n","                remaining_batches = len(batches) - completed_batches\n","\n","                print(f\"\\n\" + \"=\"*60)\n","                print(f\"üéØ RESEARCH BATCH {current_batch_num} COMPLETE!\")\n","                print(f\"=\"*60)\n","                print(f\"‚úÖ Processed: {completed_batches}/{len(batches)} batches\")\n","                print(f\"‚è≥ Remaining: {remaining_batches} batches\")\n","                print(f\"üß† Quality: GPT-4o validated ‚Üí GPT-4o-mini extracted\")\n","\n","                if remaining_batches > 0:\n","                    estimated_remaining_time = remaining_batches * (processing_time / 60)\n","                    print(f\"‚è±Ô∏è  Estimated remaining time: {estimated_remaining_time:.1f} minutes\")\n","                    print(f\"\\nüöÄ NEXT: Run this cell again for Research Batch {current_batch_num + 1}\")\n","                else:\n","                    print(f\"üéâ ALL RESEARCH BATCHES COMPLETED!\")\n","                    print(f\"üìä Ready for construction + AI/ML analysis!\")\n","\n","                print(f\"=\"*60)\n","\n","            except Exception as e:\n","                print(f\"‚ùå Error processing research batch {current_batch_num}: {e}\")\n","                print(f\"‚ö†Ô∏è  You can try running this cell again or continue to next batch\")\n","\n","        else:\n","            print(f\"‚ùå No documents to process for research batch {current_batch_num}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4c2BoSUJLQkI","executionInfo":{"status":"ok","timestamp":1751147793026,"user_tz":-600,"elapsed":1988,"user":{"displayName":"M. Reza Hosseini","userId":"13449621777993109619"}},"outputId":"aa477ff1-f754-4047-a8d5-bf893314921a"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["üöÄ INTERACTIVE RESEARCH GRAPH EXTRACTION\n","üß† Dataset: GPT-4o Validated | Extraction: GPT-4o-mini\n","üéØ Focus: Construction + AI/ML Ecosystem\n","============================================================\n","üéâ ALL RESEARCH BATCHES COMPLETED!\n","‚úÖ All 15 research batches processed successfully\n","üß† GPT-4o validated dataset ‚Üí Clean knowledge graph\n","\n","üìä FINAL RESEARCH GRAPH STATISTICS:\n","  Total Nodes: 0\n","  Total Relationships: 0\n","\n","üéØ RESEARCH METRICS:\n","  üìä Jobs processed: 282 (GPT-4o validated)\n","  üèóÔ∏è  Entities per job: 0.0\n","  üîó Relationships per job: 0.0\n","  üß† Dataset quality: Research-grade\n"]}]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.6"},"colab":{"provenance":[{"file_id":"12uLW078Ot93gxNCClcGNnXzLmpIe3yTH","timestamp":1750920294369},{"file_id":"1FdbdZv8esZxKRAF9Z2M0moT1p7NCtYgR","timestamp":1750631819584},{"file_id":"1PWkFydccl7t6ydl1bz_7dYU3tQ6VsZIH","timestamp":1749611182137}]}},"nbformat":4,"nbformat_minor":0}